{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.corpus import wordnet as wn\n",
    "from transformers import BertModel, BertTokenizer, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers_interpret import SequenceClassificationExplainer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "import shap\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import empath\n",
    "import string\n",
    "import re\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#https://github.com/cdpierse/transformers-interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = empath.Empath()\n",
    "def get_empath(text_list):\n",
    "    json_list = []\n",
    "    for i in text_list:\n",
    "        empath = lexicon.analyze(i, normalize=True)\n",
    "        json_list.append(empath)\n",
    "    return json_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "552"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from writeprints_static import WriteprintsStatic\n",
    "\n",
    "texts = [\"Colorless green ideas sleep furiously.\"]#, \"Furiously sleep ideas green colorless.\", 'James, while John had had \"had\", had had \"had had\"; \"had had\" had had a better effect on the teacher.']\n",
    "\n",
    "vec_ws = WriteprintsStatic()\n",
    "\n",
    "# The input only accepts list of English string, so there is no need to specify input type as usually did for\n",
    "# scikit-learn.\n",
    "# Output X is a scipy.sparse.csr_matrix instance\n",
    "X = vec_ws.transform(texts)\n",
    "\n",
    "# to check the feature values\n",
    "X.toarray()\n",
    "\n",
    "# to check the feature names\n",
    "wps_features=vec_ws.get_feature_names()#column labels\n",
    "len(wps_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>true-author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unknown00001</td>\n",
       "      <td>China and Britain agreed on Wednesday to relea...</td>\n",
       "      <td>candidate00046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unknown00002</td>\n",
       "      <td>The Federal Reserve may not be taking adequate...</td>\n",
       "      <td>candidate00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unknown00003</td>\n",
       "      <td>Britain's motor industry reported 1996 car reg...</td>\n",
       "      <td>candidate00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unknown00004</td>\n",
       "      <td>When the former Czechoslovak diplomat Josef Ko...</td>\n",
       "      <td>candidate00019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>unknown00005</td>\n",
       "      <td>China is building a network of major toll high...</td>\n",
       "      <td>candidate00012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>unknown02496</td>\n",
       "      <td>Britain's big banks look set to raise profits ...</td>\n",
       "      <td>candidate00018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>unknown02497</td>\n",
       "      <td>After two years of hype and euphoria about the...</td>\n",
       "      <td>candidate00047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>unknown02498</td>\n",
       "      <td>Czech annual average consumer inflation eased ...</td>\n",
       "      <td>candidate00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>unknown02499</td>\n",
       "      <td>Kellogg Co, whose profits for 1996 are under p...</td>\n",
       "      <td>candidate00037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>unknown02500</td>\n",
       "      <td>London-based international bank HSBC Holdings ...</td>\n",
       "      <td>candidate00018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                               Text   \n",
       "0     unknown00001  China and Britain agreed on Wednesday to relea...  \\\n",
       "1     unknown00002  The Federal Reserve may not be taking adequate...   \n",
       "2     unknown00003  Britain's motor industry reported 1996 car reg...   \n",
       "3     unknown00004  When the former Czechoslovak diplomat Josef Ko...   \n",
       "4     unknown00005  China is building a network of major toll high...   \n",
       "...            ...                                                ...   \n",
       "2495  unknown02496  Britain's big banks look set to raise profits ...   \n",
       "2496  unknown02497  After two years of hype and euphoria about the...   \n",
       "2497  unknown02498  Czech annual average consumer inflation eased ...   \n",
       "2498  unknown02499  Kellogg Co, whose profits for 1996 are under p...   \n",
       "2499  unknown02500  London-based international bank HSBC Holdings ...   \n",
       "\n",
       "         true-author  \n",
       "0     candidate00046  \n",
       "1     candidate00001  \n",
       "2     candidate00009  \n",
       "3     candidate00019  \n",
       "4     candidate00012  \n",
       "...              ...  \n",
       "2495  candidate00018  \n",
       "2496  candidate00047  \n",
       "2497  candidate00002  \n",
       "2498  candidate00037  \n",
       "2499  candidate00018  \n",
       "\n",
       "[2500 rows x 3 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataframe\n",
    "# data = pd.read_csv(r'Datasets\\CASIS\\casis1000.csv')\n",
    "# data = data.iloc[:100]\n",
    "# data = data.drop('ID', axis=1)\n",
    "# data\n",
    "train_data = pd.read_csv(r\"data\\c50.csv\")\n",
    "test_data = pd.read_csv(r\"data\\c50_Test.csv\")\n",
    "test_data = test_data.drop(test_data.columns[0], axis=1)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true-author</th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>candidate00046</td>\n",
       "      <td>unknown00001</td>\n",
       "      <td>China and Britain agreed on Wednesday to relea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>unknown00002</td>\n",
       "      <td>The Federal Reserve may not be taking adequate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>candidate00009</td>\n",
       "      <td>unknown00003</td>\n",
       "      <td>Britain's motor industry reported 1996 car reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>candidate00019</td>\n",
       "      <td>unknown00004</td>\n",
       "      <td>When the former Czechoslovak diplomat Josef Ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>candidate00012</td>\n",
       "      <td>unknown00005</td>\n",
       "      <td>China is building a network of major toll high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>candidate00018</td>\n",
       "      <td>unknown02496</td>\n",
       "      <td>Britain's big banks look set to raise profits ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>candidate00047</td>\n",
       "      <td>unknown02497</td>\n",
       "      <td>After two years of hype and euphoria about the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>candidate00002</td>\n",
       "      <td>unknown02498</td>\n",
       "      <td>Czech annual average consumer inflation eased ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>candidate00037</td>\n",
       "      <td>unknown02499</td>\n",
       "      <td>Kellogg Co, whose profits for 1996 are under p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>candidate00018</td>\n",
       "      <td>unknown02500</td>\n",
       "      <td>London-based international bank HSBC Holdings ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         true-author            ID   \n",
       "0     candidate00046  unknown00001  \\\n",
       "1     candidate00001  unknown00002   \n",
       "2     candidate00009  unknown00003   \n",
       "3     candidate00019  unknown00004   \n",
       "4     candidate00012  unknown00005   \n",
       "...              ...           ...   \n",
       "2495  candidate00018  unknown02496   \n",
       "2496  candidate00047  unknown02497   \n",
       "2497  candidate00002  unknown02498   \n",
       "2498  candidate00037  unknown02499   \n",
       "2499  candidate00018  unknown02500   \n",
       "\n",
       "                                                   Text  \n",
       "0     China and Britain agreed on Wednesday to relea...  \n",
       "1     The Federal Reserve may not be taking adequate...  \n",
       "2     Britain's motor industry reported 1996 car reg...  \n",
       "3     When the former Czechoslovak diplomat Josef Ko...  \n",
       "4     China is building a network of major toll high...  \n",
       "...                                                 ...  \n",
       "2495  Britain's big banks look set to raise profits ...  \n",
       "2496  After two years of hype and euphoria about the...  \n",
       "2497  Czech annual average consumer inflation eased ...  \n",
       "2498  Kellogg Co, whose profits for 1996 are under p...  \n",
       "2499  London-based international bank HSBC Holdings ...  \n",
       "\n",
       "[2500 rows x 3 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = list(test_data.columns)\n",
    "cols = [cols[-1]] + cols[:-1]\n",
    "test_data = test_data.reindex(columns=cols)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>candidate00046</td>\n",
       "      <td>unknown00001</td>\n",
       "      <td>China and Britain agreed on Wednesday to relea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>unknown00002</td>\n",
       "      <td>The Federal Reserve may not be taking adequate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>candidate00009</td>\n",
       "      <td>unknown00003</td>\n",
       "      <td>Britain's motor industry reported 1996 car reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>candidate00019</td>\n",
       "      <td>unknown00004</td>\n",
       "      <td>When the former Czechoslovak diplomat Josef Ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>candidate00012</td>\n",
       "      <td>unknown00005</td>\n",
       "      <td>China is building a network of major toll high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>candidate00018</td>\n",
       "      <td>unknown02496</td>\n",
       "      <td>Britain's big banks look set to raise profits ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>candidate00047</td>\n",
       "      <td>unknown02497</td>\n",
       "      <td>After two years of hype and euphoria about the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>candidate00002</td>\n",
       "      <td>unknown02498</td>\n",
       "      <td>Czech annual average consumer inflation eased ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>candidate00037</td>\n",
       "      <td>unknown02499</td>\n",
       "      <td>Kellogg Co, whose profits for 1996 are under p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>candidate00018</td>\n",
       "      <td>unknown02500</td>\n",
       "      <td>London-based international bank HSBC Holdings ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Author            ID   \n",
       "0     candidate00046  unknown00001  \\\n",
       "1     candidate00001  unknown00002   \n",
       "2     candidate00009  unknown00003   \n",
       "3     candidate00019  unknown00004   \n",
       "4     candidate00012  unknown00005   \n",
       "...              ...           ...   \n",
       "2495  candidate00018  unknown02496   \n",
       "2496  candidate00047  unknown02497   \n",
       "2497  candidate00002  unknown02498   \n",
       "2498  candidate00037  unknown02499   \n",
       "2499  candidate00018  unknown02500   \n",
       "\n",
       "                                                   Text  \n",
       "0     China and Britain agreed on Wednesday to relea...  \n",
       "1     The Federal Reserve may not be taking adequate...  \n",
       "2     Britain's motor industry reported 1996 car reg...  \n",
       "3     When the former Czechoslovak diplomat Josef Ko...  \n",
       "4     China is building a network of major toll high...  \n",
       "...                                                 ...  \n",
       "2495  Britain's big banks look set to raise profits ...  \n",
       "2496  After two years of hype and euphoria about the...  \n",
       "2497  Czech annual average consumer inflation eased ...  \n",
       "2498  Kellogg Co, whose profits for 1996 are under p...  \n",
       "2499  London-based international bank HSBC Holdings ...  \n",
       "\n",
       "[2500 rows x 3 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data.rename(columns={'true-author': 'Author'})\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>candidate00046</td>\n",
       "      <td>China and Britain agreed on Wednesday to relea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>The Federal Reserve may not be taking adequate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>candidate00009</td>\n",
       "      <td>Britain's motor industry reported 1996 car reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>candidate00019</td>\n",
       "      <td>When the former Czechoslovak diplomat Josef Ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>candidate00012</td>\n",
       "      <td>China is building a network of major toll high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>candidate00018</td>\n",
       "      <td>Britain's big banks look set to raise profits ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>candidate00047</td>\n",
       "      <td>After two years of hype and euphoria about the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>candidate00002</td>\n",
       "      <td>Czech annual average consumer inflation eased ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>candidate00037</td>\n",
       "      <td>Kellogg Co, whose profits for 1996 are under p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>candidate00018</td>\n",
       "      <td>London-based international bank HSBC Holdings ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Author                                               Text\n",
       "0     candidate00046  China and Britain agreed on Wednesday to relea...\n",
       "1     candidate00001  The Federal Reserve may not be taking adequate...\n",
       "2     candidate00009  Britain's motor industry reported 1996 car reg...\n",
       "3     candidate00019  When the former Czechoslovak diplomat Josef Ko...\n",
       "4     candidate00012  China is building a network of major toll high...\n",
       "...              ...                                                ...\n",
       "2495  candidate00018  Britain's big banks look set to raise profits ...\n",
       "2496  candidate00047  After two years of hype and euphoria about the...\n",
       "2497  candidate00002  Czech annual average consumer inflation eased ...\n",
       "2498  candidate00037  Kellogg Co, whose profits for 1996 are under p...\n",
       "2499  candidate00018  London-based international bank HSBC Holdings ...\n",
       "\n",
       "[2500 rows x 2 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data.drop(columns='ID', axis=1)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>The Internet may be overflowing with new techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>The U.S. Postal Service announced Wednesday a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>Elementary school students with access to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>An influential Internet organisation has backe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>An influential Internet organisation has backe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>candidate00018</td>\n",
       "      <td>Britain's big banks look set to raise profits ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>candidate00047</td>\n",
       "      <td>After two years of hype and euphoria about the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>candidate00002</td>\n",
       "      <td>Czech annual average consumer inflation eased ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>candidate00037</td>\n",
       "      <td>Kellogg Co, whose profits for 1996 are under p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>candidate00018</td>\n",
       "      <td>London-based international bank HSBC Holdings ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Author                                               Text\n",
       "0     candidate00001  The Internet may be overflowing with new techn...\n",
       "1     candidate00001  The U.S. Postal Service announced Wednesday a ...\n",
       "2     candidate00001  Elementary school students with access to the ...\n",
       "3     candidate00001  An influential Internet organisation has backe...\n",
       "4     candidate00001  An influential Internet organisation has backe...\n",
       "...              ...                                                ...\n",
       "2495  candidate00018  Britain's big banks look set to raise profits ...\n",
       "2496  candidate00047  After two years of hype and euphoria about the...\n",
       "2497  candidate00002  Czech annual average consumer inflation eased ...\n",
       "2498  candidate00037  Kellogg Co, whose profits for 1996 are under p...\n",
       "2499  candidate00018  London-based international bank HSBC Holdings ...\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = pd.concat([train_data, test_data])\n",
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove leading and trailing whitespace\n",
    "full_data['Text'] = full_data['Text'].str.strip()\n",
    "\n",
    "# replace multiple spaces with a single space\n",
    "full_data['Text'] = full_data['Text'].str.replace(r'\\s+', ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_emails = lambda x: re.sub(r'\\S+@\\S+', '', x)\n",
    "remove_urls = lambda x: re.sub(r'http\\S+', '', x)\n",
    "\n",
    "full_data['Text'] = full_data['Text'].apply(remove_emails).apply(remove_urls)\n",
    "full_data['Text'] = full_data['Text'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x.lower()))\n",
    "full_data['Text'] = full_data['Text'].str.replace('\\n', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>candidate00046</td>\n",
       "      <td>China and Britain agreed on Wednesday to relea...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>The Federal Reserve may not be taking adequate...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>candidate00009</td>\n",
       "      <td>Britain's motor industry reported 1996 car reg...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>candidate00019</td>\n",
       "      <td>When the former Czechoslovak diplomat Josef Ko...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>candidate00012</td>\n",
       "      <td>China is building a network of major toll high...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>candidate00018</td>\n",
       "      <td>Britain's big banks look set to raise profits ...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>candidate00047</td>\n",
       "      <td>After two years of hype and euphoria about the...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>candidate00002</td>\n",
       "      <td>Czech annual average consumer inflation eased ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>candidate00037</td>\n",
       "      <td>Kellogg Co, whose profits for 1996 are under p...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>candidate00018</td>\n",
       "      <td>London-based international bank HSBC Holdings ...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Author                                               Text   \n",
       "0     candidate00046  China and Britain agreed on Wednesday to relea...  \\\n",
       "1     candidate00001  The Federal Reserve may not be taking adequate...   \n",
       "2     candidate00009  Britain's motor industry reported 1996 car reg...   \n",
       "3     candidate00019  When the former Czechoslovak diplomat Josef Ko...   \n",
       "4     candidate00012  China is building a network of major toll high...   \n",
       "...              ...                                                ...   \n",
       "2495  candidate00018  Britain's big banks look set to raise profits ...   \n",
       "2496  candidate00047  After two years of hype and euphoria about the...   \n",
       "2497  candidate00002  Czech annual average consumer inflation eased ...   \n",
       "2498  candidate00037  Kellogg Co, whose profits for 1996 are under p...   \n",
       "2499  candidate00018  London-based international bank HSBC Holdings ...   \n",
       "\n",
       "      word_count  \n",
       "0             36  \n",
       "1             37  \n",
       "2             35  \n",
       "3             35  \n",
       "4             35  \n",
       "...          ...  \n",
       "2495          37  \n",
       "2496          31  \n",
       "2497          19  \n",
       "2498          37  \n",
       "2499          23  \n",
       "\n",
       "[2500 rows x 3 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_func = lambda x: len(x.split())\n",
    "train_data['word_count'] = train_data['Text'].apply(word_count_func)\n",
    "test_data['word_count'] = test_data['Text'].apply(word_count_func)\n",
    "test_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# # load the pre-trained FastText model\n",
    "# model_path = r\"wiki-news-300d-1M-subword.vec\\wiki-news-300d-1M-subword.vec\"\n",
    "# model = KeyedVectors.load_word2vec_format(model_path, binary=False)\n",
    "\n",
    "# # check the size of the vocabulary and the vector size\n",
    "# print(f\"Vocabulary size: {len(model.index_to_key)}\")\n",
    "# print(f\"Vector size: {model.vector_size}\")\n",
    "# len(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = train_data['word_count'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "           output_hidden_states = True,)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "device = 'cuda'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def bert_sent_embedding(doc, model):\n",
    "# #     flatten_emb = []\n",
    "# #     sentences = doc.split('.')\n",
    "# #     embeddings = model.encode(sentences)\n",
    "# #     for sent in embeddings:\n",
    "# #         flatten_emb.extend(sent)\n",
    "# #     return flatten_emb\n",
    "\n",
    "# def bert_text_preparation(text, tokenizer):\n",
    "#   \"\"\"\n",
    "#   Preprocesses text input in a way that BERT can interpret.\n",
    "#   \"\"\"\n",
    "#   marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "#   tokenized_text = tokenizer.tokenize(marked_text)\n",
    "#   indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "#   segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "#   # convert inputs to tensors\n",
    "#   tokens_tensor = torch.tensor([indexed_tokens])\n",
    "#   segments_tensor = torch.tensor([segments_ids])\n",
    "\n",
    "#   return tokenized_text, tokens_tensor, segments_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_doc_embs = []\n",
    "# for doc in train_docs:\n",
    "#     train_doc_embs.append(bert_sent_embedding(doc, model))\n",
    "# train_doc_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_doc_embs = []\n",
    "# for doc in test_docs:\n",
    "#     test_doc_embs.append(bert_sent_embedding(doc, model))\n",
    "# test_doc_embs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = train_data['Text'].to_list()\n",
    "train_docs_labels = train_data['Author'].to_list()\n",
    "test_docs = test_data['Text'].to_list()\n",
    "test_docs_labels = test_data['true-author'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = [tokenizer.tokenize(doc) for doc in train_docs]\n",
    "test_tokens = [tokenizer.tokenize(doc) for doc in test_docs]\n",
    "# Add special tokens [CLS] and [SEP] to the train and test tokens\n",
    "train_tokens = [['[CLS]'] + tokens + ['[SEP]'] for tokens in train_tokens]\n",
    "test_tokens = [['[CLS]'] + tokens + ['[SEP]'] for tokens in test_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the train and test tokens to token IDs\n",
    "train_input_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in train_tokens]\n",
    "test_input_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in test_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the train and test input IDs to the same length\n",
    "max_len = max(len(ids) for ids in train_input_ids + test_input_ids)\n",
    "train_input_ids = [ids + [0] * (max_len - len(ids)) for ids in train_input_ids]\n",
    "test_input_ids = [ids + [0] * (max_len - len(ids)) for ids in test_input_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the train and test input IDs to PyTorch tensors\n",
    "train_input_ids = torch.tensor(train_input_ids).to(device)\n",
    "test_input_ids = torch.tensor(test_input_ids).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size and compute number of batches\n",
    "batch_size = 5\n",
    "num_train_batches = len(train_input_ids) // batch_size\n",
    "num_test_batches = len(test_input_ids) // batch_size\n",
    "\n",
    "# Create train and test data loaders\n",
    "train_data = torch.utils.data.DataLoader(\n",
    "    [(train_input_ids[i*batch_size:(i+1)*batch_size], i) for i in range(num_train_batches)],\n",
    "    batch_size=None,\n",
    "    shuffle=False\n",
    ")\n",
    "test_data = torch.utils.data.DataLoader(\n",
    "    [(test_input_ids[i*batch_size:(i+1)*batch_size], i) for i in range(num_test_batches)],\n",
    "    batch_size=None,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Compute the BERT embeddings for the train and test documents\n",
    "with torch.no_grad():\n",
    "    train_embeddings = []\n",
    "    test_embeddings = []\n",
    "    for batch, _ in train_data:\n",
    "        batch_embeddings = model(batch)[0]\n",
    "        train_embeddings.append(batch_embeddings)\n",
    "    for batch, _ in test_data:\n",
    "        batch_embeddings = model(batch)[0]\n",
    "        test_embeddings.append(batch_embeddings)\n",
    "    train_embeddings = torch.cat(train_embeddings, dim=0)\n",
    "    test_embeddings = torch.cat(test_embeddings, dim=0)\n",
    "\n",
    "# The shape of the document-level embeddings is (num_docs, embedding_dim)\n",
    "# You can use these embeddings to train a classifier or for other downstream tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2500, 88, 768]), torch.Size([2500, 88, 768]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings.shape, test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the document-level embeddings, you can average the embeddings across the token dimension\n",
    "train_doc_embeddings = torch.mean(train_embeddings, dim=1)\n",
    "test_doc_embeddings = torch.mean(test_embeddings, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the embeddings into a 2D array\n",
    "num_sentences, max_seq_len, embedding_dim = train_embeddings.shape\n",
    "train_embeddings = train_embeddings.view(num_sentences, max_seq_len * embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences, max_seq_len, embedding_dim = test_embeddings.shape\n",
    "test_embeddings = test_embeddings.view(num_sentences, max_seq_len * embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2500, 67584]), torch.Size([2500, 67584]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings.shape, test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings.cpu().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = list(train_data['Text'])\n",
    "train_auth = list(train_data['Author'])\n",
    "test_text = list(test_data['Text'])\n",
    "test_auth = list(test_data['Author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emp = get_empath(train_text)\n",
    "test_emp = get_empath(test_text)\n",
    "keys = list(train_emp[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emp_df = pd.json_normalize(train_emp)\n",
    "train_emp_df.columns = keys\n",
    "test_emp_df = pd.json_normalize(test_emp)\n",
    "test_emp_df.columns = keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_ws_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37360\\3598800593.py\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# pipe.score(test_text, test_auth)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ws_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_auth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_ws_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_auth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_ws_df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "vec = TfidfVectorizer(min_df=3, stop_words='english',\n",
    "                      ngram_range=(1, 2))\n",
    "pca = PCA(n_components=200, random_state=42)\n",
    "svd = TruncatedSVD(n_components=75, n_iter=100, random_state=42)\n",
    "clf = NuSVC(random_state=42)\n",
    "pipe = make_pipeline(svd,clf)\n",
    "\n",
    "# clf = SVC(C=150, gamma=2e-2, probability=True)\n",
    "# pipe = make_pipeline(lsa, clf)\n",
    "# pipe.fit(train_text, train_auth)\n",
    "# pipe.score(test_text, test_auth)\n",
    "\n",
    "pipe.fit(train_ws_df, train_auth)\n",
    "pipe.score(test_ws_df, test_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [01:02<00:00,  2.17s/it]\n"
     ]
    }
   ],
   "source": [
    "lc = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = lc.fit(train_emp_df, test_emp_df, train_auth, test_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ExtraTreesClassifier</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>None</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>None</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifierCV</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>None</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifier</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>None</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearDiscriminantAnalysis</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>None</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>None</td>\n",
       "      <td>0.16</td>\n",
       "      <td>4.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSVC</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>None</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>None</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingClassifier</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>None</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>None</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>None</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearestCentroid</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>None</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>None</td>\n",
       "      <td>0.14</td>\n",
       "      <td>4.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CalibratedClassifierCV</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>None</td>\n",
       "      <td>0.10</td>\n",
       "      <td>28.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>None</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>None</td>\n",
       "      <td>0.12</td>\n",
       "      <td>7.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveClassifier</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>None</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>None</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>None</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>None</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>None</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuadraticDiscriminantAnalysis</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>None</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  Balanced Accuracy ROC AUC  F1 Score   \n",
       "Model                                                                          \n",
       "ExtraTreesClassifier               0.18               0.18    None      0.18  \\\n",
       "RandomForestClassifier             0.17               0.17    None      0.17   \n",
       "RidgeClassifierCV                  0.17               0.17    None      0.16   \n",
       "RidgeClassifier                    0.17               0.17    None      0.16   \n",
       "LinearDiscriminantAnalysis         0.17               0.17    None      0.17   \n",
       "LGBMClassifier                     0.16               0.16    None      0.16   \n",
       "NuSVC                              0.16               0.16    None      0.16   \n",
       "BernoulliNB                        0.15               0.15    None      0.14   \n",
       "BaggingClassifier                  0.15               0.15    None      0.14   \n",
       "SVC                                0.15               0.15    None      0.15   \n",
       "LogisticRegression                 0.15               0.15    None      0.15   \n",
       "NearestCentroid                    0.15               0.15    None      0.14   \n",
       "SGDClassifier                      0.13               0.13    None      0.14   \n",
       "CalibratedClassifierCV             0.13               0.13    None      0.10   \n",
       "Perceptron                         0.12               0.12    None      0.12   \n",
       "LinearSVC                          0.12               0.12    None      0.12   \n",
       "PassiveAggressiveClassifier        0.12               0.12    None      0.12   \n",
       "DecisionTreeClassifier             0.11               0.11    None      0.11   \n",
       "KNeighborsClassifier               0.09               0.09    None      0.09   \n",
       "ExtraTreeClassifier                0.08               0.08    None      0.08   \n",
       "GaussianNB                         0.06               0.06    None      0.05   \n",
       "AdaBoostClassifier                 0.03               0.03    None      0.01   \n",
       "QuadraticDiscriminantAnalysis      0.03               0.03    None      0.02   \n",
       "DummyClassifier                    0.02               0.02    None      0.00   \n",
       "\n",
       "                               Time Taken  \n",
       "Model                                      \n",
       "ExtraTreesClassifier                 1.38  \n",
       "RandomForestClassifier               1.71  \n",
       "RidgeClassifierCV                    0.23  \n",
       "RidgeClassifier                      0.11  \n",
       "LinearDiscriminantAnalysis           0.27  \n",
       "LGBMClassifier                       4.78  \n",
       "NuSVC                                2.50  \n",
       "BernoulliNB                          0.17  \n",
       "BaggingClassifier                    2.07  \n",
       "SVC                                  2.16  \n",
       "LogisticRegression                   0.64  \n",
       "NearestCentroid                      0.10  \n",
       "SGDClassifier                        4.70  \n",
       "CalibratedClassifierCV              28.56  \n",
       "Perceptron                           0.70  \n",
       "LinearSVC                            7.29  \n",
       "PassiveAggressiveClassifier          1.18  \n",
       "DecisionTreeClassifier               0.21  \n",
       "KNeighborsClassifier                 0.27  \n",
       "ExtraTreeClassifier                  0.07  \n",
       "GaussianNB                           0.29  \n",
       "AdaBoostClassifier                   1.68  \n",
       "QuadraticDiscriminantAnalysis        0.53  \n",
       "DummyClassifier                      0.07  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ws = vec_ws.transform(train_text)\n",
    "test_ws = vec_ws.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "552"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ws_arr = train_ws.toarray()\n",
    "test_ws_arr = test_ws.toarray()\n",
    "test_ws_arr\n",
    "\n",
    "train_ws_df = pd.DataFrame(columns=wps_features)\n",
    "test_ws_df = pd.DataFrame(columns=wps_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ws_df =pd.DataFrame(train_ws_arr, columns= wps_features)\n",
    "test_ws_df =pd.DataFrame(train_ws_arr, columns= wps_features)\n",
    "test_ws_df.all(axis=0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tuple' object has no attribute '__name__'\n",
      "Invalid Classifier(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [01:35<00:00,  3.30s/it]\n"
     ]
    }
   ],
   "source": [
    "models, predictions = lc.fit(train_ws_df, test_ws_df, train_auth, test_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Accuracy, Balanced Accuracy, ROC AUC, F1 Score, Time Taken]\n",
       "Index: []"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pipe.predict(test_text)\n",
    "test_tfidf = vec.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10319855, 0.02477061, 0.01826841, 0.01676215, 0.03019434,\n",
       "       0.01982165, 0.0211624 , 0.01289245, 0.01825405, 0.02305973,\n",
       "       0.02257832, 0.01733536, 0.01220541, 0.01865016, 0.01555266,\n",
       "       0.01485771, 0.01564377, 0.02860689, 0.02181543, 0.01459736,\n",
       "       0.01660711, 0.01651339, 0.02015031, 0.02296129, 0.01542159,\n",
       "       0.01677756, 0.01751102, 0.01349318, 0.01433777, 0.01511142,\n",
       "       0.01667602, 0.02225941, 0.01345882, 0.02131578, 0.01678983,\n",
       "       0.01530981, 0.02278673, 0.01234125, 0.01574953, 0.01531201,\n",
       "       0.02662716, 0.02032261, 0.01810954, 0.01995325, 0.01737444,\n",
       "       0.01465417, 0.01637809, 0.01890779, 0.01613328, 0.02042843])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pipe.predict_proba(test_text)\n",
    "t[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifiation report\n"
     ]
    }
   ],
   "source": [
    "print('classifiation report')\n",
    "# print(classification_report(test_auth, test_pred,  target_names=train_data['Author'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res_df = pd.DataFrame({'true_label': test_auth, 'predicted_label': test_pred})\n",
    "test_res_df\n",
    "correct_pred_df = test_res_df[test_res_df[\"true_label\"] == test_res_df[\"predicted_label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>candidate00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>candidate00012</td>\n",
       "      <td>candidate00012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>candidate00033</td>\n",
       "      <td>candidate00033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>candidate00024</td>\n",
       "      <td>candidate00024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>candidate00019</td>\n",
       "      <td>candidate00019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>candidate00048</td>\n",
       "      <td>candidate00048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>candidate00012</td>\n",
       "      <td>candidate00012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>candidate00047</td>\n",
       "      <td>candidate00047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>candidate00017</td>\n",
       "      <td>candidate00017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>candidate00011</td>\n",
       "      <td>candidate00011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>candidate00025</td>\n",
       "      <td>candidate00025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>candidate00023</td>\n",
       "      <td>candidate00023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>candidate00029</td>\n",
       "      <td>candidate00029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>candidate00024</td>\n",
       "      <td>candidate00024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>candidate00048</td>\n",
       "      <td>candidate00048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>candidate00049</td>\n",
       "      <td>candidate00049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>candidate00042</td>\n",
       "      <td>candidate00042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>candidate00032</td>\n",
       "      <td>candidate00032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>candidate00049</td>\n",
       "      <td>candidate00049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>candidate00049</td>\n",
       "      <td>candidate00049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        true_label predicted_label\n",
       "1   candidate00001  candidate00001\n",
       "4   candidate00012  candidate00012\n",
       "5   candidate00033  candidate00033\n",
       "6   candidate00024  candidate00024\n",
       "8   candidate00019  candidate00019\n",
       "9   candidate00048  candidate00048\n",
       "11  candidate00012  candidate00012\n",
       "12  candidate00047  candidate00047\n",
       "14  candidate00017  candidate00017\n",
       "17  candidate00011  candidate00011\n",
       "19  candidate00025  candidate00025\n",
       "20  candidate00023  candidate00023\n",
       "21  candidate00029  candidate00029\n",
       "22  candidate00024  candidate00024\n",
       "23  candidate00048  candidate00048\n",
       "24  candidate00049  candidate00049\n",
       "25  candidate00042  candidate00042\n",
       "27  candidate00032  candidate00032\n",
       "30  candidate00049  candidate00049\n",
       "31  candidate00049  candidate00049"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_pred_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction(doc):\n",
    "    y_pred = pipe.predict_proba([doc])[0]\n",
    "    for target, prob in zip(set(train_auth), y_pred):\n",
    "        print(\"{:.3f} {}\".format(prob, target))\n",
    "\n",
    "# doc = test_text[0]\n",
    "# print_prediction(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the predicted labels with the true labels to identify the misclassified instances\n",
    "misclassified_indices = [i for i in range(len(test_auth)) if test_auth[i] != test_pred[i]]\n",
    "\n",
    "# retrieve the texts corresponding to the misclassified instances\n",
    "misclassified_texts = [test_text[i] for i in misclassified_indices]\n",
    "\n",
    "classified_indices = [i for i in range(len(test_auth)) if test_auth[i] == test_pred[i]]\n",
    "\n",
    "classified_texts = [test_text[i] for i in classified_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1313"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classified_texts)\n",
    "# 1313/2500\n",
    "# classified_indices #1,4,5,6,8,9,11,12,14,17,19,20,21,22,23,24,25,27,30"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_data['Author'].unique()\n",
    "explainer = LimeTextExplainer(class_names=classes)\n",
    "exp_text =test_text[4]\n",
    "lime_exp = explainer.explain_instance(exp_text, pipe.predict_proba)#, num_features=(len(exp_text)//5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37360\\596331717.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# single_pred= pipe.predict(exp_text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# single_pred= pipe.predict(exp_text)\n",
    "test_pred[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'candidate00001'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fig = exp.as_pyplot_figure()\n",
    "# classes\n",
    "test_auth[1]\n",
    "# lime_exp.as_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_exp.show_in_notebook(text=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_exp = shap.Explainer(pipe.predict_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the SHAP values for the text instance\n",
    "shap_values = shap_exp(test_tfidf[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the SHAP values as a bar chart\n",
    "shap.summary_plot(shap_values[1], exp_text, feature_names=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_function(sentence, model):\n",
    "    # Tokenize the sentence into individual words\n",
    "    tokens = sentence.split()\n",
    "    # Get the embedding for each word in the sentence\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in model:\n",
    "            embeddings.append(model[token])\n",
    "    # Calculate the average embedding for the sentence\n",
    "    if len(embeddings) > 0:\n",
    "        sentence_embedding = np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        sentence_embedding = np.zeros((300,))\n",
    "    return sentence_embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(sentence):\n",
    "    # Tokenize the sentence into individual words\n",
    "    tokens = sentence.split()\n",
    "    # Get the embedding for each word in the sentence\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in model:\n",
    "            embeddings.append(model[token])\n",
    "    # Calculate the average embedding for the sentence\n",
    "    if len(embeddings) > 0:\n",
    "        sentence_embedding = np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        sentence_embedding = np.zeros((300,))\n",
    "    return sentence_embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers_Interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = pd.Categorical(full_data['Author']).categories\n",
    "full_data['encoded_Author'] = pd.Categorical(full_data['Author'], categories=categories).codes\n",
    "\n",
    "full_texts = full_data['Text'].to_list()\n",
    "full_labels = full_data['encoded_Author'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 1000, 1000, 3000, 1000, 1000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FOR FULL_DATA\n",
    "X = full_data['Text']#.drop(['Author', 'encoded_author'], axis=1)\n",
    "y = full_data['encoded_Author']\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split train into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "len(X_train), len(X_test), len(X_val), len(y_train), len(y_test), len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34152\\1998547842.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# df_balanced['encoded_id'] = df_balanced['id'].astype('category').cat.codes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# get unique categories for Author in both dataframes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcategories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Author'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'true-author'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# assign categorical variable to both dataframes using the same categories\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "# MAX_LENGTH = train_data['word_count'].max()\n",
    "\n",
    "# df_balanced['encoded_id'] = df_balanced['id'].astype('category').cat.codes\n",
    "# get unique categories for Author in both dataframes\n",
    "categories = pd.Categorical(pd.concat([train_data['Author'], test_data['true-author']], ignore_index=True)).categories\n",
    "\n",
    "# assign categorical variable to both dataframes using the same categories\n",
    "train_data['encoded_Author'] = pd.Categorical(train_data['Author'], categories=categories).codes\n",
    "test_data['encoded_Author'] = pd.Categorical(test_data['true-author'], categories=categories).codes\n",
    "\n",
    "\n",
    "# Define the training and validation data\n",
    "# texts = df_balanced['text'].to_list()\n",
    "# labels = df_balanced['encoded_id'].to_list()\n",
    "\n",
    "train_texts = train_data['Text'].to_list()\n",
    "train_labels = train_data['encoded_Author'].to_list()\n",
    "#split test into test and validation\n",
    "test_texts = test_data['Text'].to_list()\n",
    "test_labels = test_data['encoded_Author'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2000, 500, 2000)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split test into test and validation\n",
    "test_texts = test_data['Text'].to_list()\n",
    "test_labels = test_data['encoded_Author'].to_list()\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(test_texts, test_labels, test_size=0.8, random_state=42, stratify=test_labels)\n",
    "len(X_test), len(X_val), len(y_test), len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34152\\2023856400.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mN_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoded_Author'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#len(train_data['Author'].unique())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDistilBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'distilbert-base-uncased'\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mdo_lower_case\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# load the saved model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'full_data' is not defined"
     ]
    }
   ],
   "source": [
    "N_labels = len(full_data['encoded_Author'].unique())#len(train_data['Author'].unique())\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased',  do_lower_case=True)\n",
    "\n",
    "# load the saved model\n",
    "saved_model_path = r\"distilbert_finetuned_c50\\distilbert_finetuned_c50_35.pt\"\n",
    "saved_model = torch.load(saved_model_path, map_location=torch.device('cpu'))[\"model_state_dict\"]\n",
    "\n",
    "# get the shape of the current model's classifier layer\n",
    "current_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=N_labels)\n",
    "current_weight_shape = current_model.classifier.weight.shape\n",
    "current_bias_shape = current_model.classifier.bias.shape\n",
    "\n",
    "# modify the saved model's classifier layer\n",
    "saved_model[\"classifier.weight\"] = saved_model[\"classifier.weight\"][:current_weight_shape[0], :]\n",
    "saved_model[\"classifier.bias\"] = saved_model[\"classifier.bias\"][:current_bias_shape[0]]\n",
    "\n",
    "# load the modified saved model into the current model\n",
    "current_model.to(device)\n",
    "current_model.load_state_dict(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25408\\4138415505.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1501\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrue_auth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1501\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcls_explainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequenceClassificationExplainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, custom_labels=set(y_test))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mword_attributions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls_explainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "text = str(X_test[1501])\n",
    "true_auth = y_test[1501]\n",
    "\n",
    "cls_explainer = SequenceClassificationExplainer(current_model, tokenizer)#, custom_labels=set(y_test))\n",
    "word_attributions = cls_explainer(text)\n",
    "del word_attributions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Actual: {true_auth}, Predicted: {cls_explainer.predicted_class_name}\")\n",
    "# #predicted starts at 0 so +1\n",
    "# print(f\"Text: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_explainer.visualize(true_class=true_auth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribution Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attribution_score(word_attributions):\n",
    "    total = 0\n",
    "    for _, value in word_attributions:\n",
    "        total += value\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_attribution_score = get_attribution_score(word_attributions)\n",
    "# original_attribution_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recombine Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_words(text, word_attributions):\n",
    "    # get list of tokens for the input text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # initialize list to store full words\n",
    "    full_words = []\n",
    "    full_scores = []\n",
    "    # del word_attributions[-1]\n",
    "    # print(word_attributions)\n",
    "    # iterate over word_attributions and map subwords to full words\n",
    "    for i, attrib in enumerate(word_attributions):\n",
    "        # attrib_list =list(attrib)\n",
    "        if i == len(tokens): # ignore padding tokens\n",
    "            break\n",
    "        elif tokens[i].startswith(\"##\"): # subword token\n",
    "            #remove ## and add to last word of full_words\n",
    "            full_words[-1] += tokenizer.convert_tokens_to_string([tokens[i]])[2:]\n",
    "            # full_words[-1] = (full_words[-1] + tokenizer.convert_tokens_to_string([tokens[i]])[2:], full_words[-1] + attrib[1])\n",
    "            # print(full_scores[-1], tokens[i])\n",
    "            full_scores[-1] += attrib[1]\n",
    "            # print( attrib[1])\n",
    "            # print(full_words[-1][1] + attrib[1])\n",
    "            # full_words [-1] = (full_words[-1][1] + attrib)\n",
    "        else: # whole word token\n",
    "            #add it as a separate word\n",
    "            full_words.append(tokenizer.convert_tokens_to_string([tokens[i]]))#, attrib[1]))\n",
    "            full_scores.append(attrib[1])\n",
    "            # print(full_scores[-1])\n",
    "            # print(full_words[-1])\n",
    "    full_word_attributions = list(zip(full_words, full_scores))\n",
    "    # print(full_words)\n",
    "    # print(full_scores)\n",
    "    return full_word_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_word_attributions = get_full_words(text, word_attributions)\n",
    "# full_word_attributions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Top Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "def get_topk(text, word_attributions):\n",
    "    recombined_words = get_full_words(text, word_attributions)\n",
    "    top_words = []\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    doc = nlp(text) #the most recent sentence form; keep to revert back to after subbing a word\n",
    "    doc_ents = [str(ent) for ent in doc.ents] #get the entities to skip over\n",
    "\n",
    "\n",
    "\n",
    "    sorted_words = sorted(recombined_words, key=lambda x: x[1], reverse=True)\n",
    "    positive_words = [t[0] for t in sorted_words if t[1] > 0]\n",
    "\n",
    "    for word in positive_words: #loop through top k words\n",
    "        if word in doc_ents or word in stop_words or any(punc in word for punc in string.punctuation): #if key is an entity, skip it\n",
    "            # print(\"NER\")\n",
    "            continue\n",
    "        else:\n",
    "            top_words.append(word)\n",
    "    \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_words = get_topk(text, full_word_attributions)\n",
    "# top_words, text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Synonyms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to get the top k synonyms for a word\n",
    "def get_synonyms_wn(word, k=5):\n",
    "    # Get the synsets for the word\n",
    "    synsets = wordnet.synsets(word)\n",
    "    # Get the lemmas for each synset and add them to a set\n",
    "    synonyms = set()\n",
    "    for synset in synsets:\n",
    "        for lemma in synset.lemmas():\n",
    "            # Lemmatize the lemma and add it to the set of synonyms\n",
    "            lemma = lemmatizer.lemmatize(lemma.name())\n",
    "            if lemma != word:\n",
    "                synonyms.add(lemma)\n",
    "    # Convert the set to a list and return the top k synonyms\n",
    "    synonyms = [syn.replace(\"_\", \" \") for syn in synonyms]\n",
    "    return list(synonyms)[:k]\n",
    "\n",
    "\n",
    "def get_syn_dict_wn(words):\n",
    "    # Dictionary to hold the synonyms\n",
    "    synonyms_dict = {}\n",
    "\n",
    "    # Get the synonyms for each word and add them to the dictionary\n",
    "    for word in words:\n",
    "        synonyms = get_synonyms_wn(word)\n",
    "        synonyms_dict[word] = synonyms\n",
    "\n",
    "    # Print the dictionary\n",
    "    # print(synonyms_dict)\n",
    "    return synonyms_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syn_dict = get_syn_dict_wn(top_words)\n",
    "# syn_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText Model with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 999994\n",
      "Vector size: 300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "999994"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# load the pre-trained FastText model\n",
    "vec_model_path = r\"wiki-news-300d-1M-subword.vec\\wiki-news-300d-1M-subword.vec\"\n",
    "vec_model = KeyedVectors.load_word2vec_format(vec_model_path, binary=False)\n",
    "\n",
    "# check the size of the vocabulary and the vector size\n",
    "print(f\"Vocabulary size: {len(vec_model.index_to_key)}\")\n",
    "print(f\"Vector size: {vec_model.vector_size}\")\n",
    "len(vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# load the FastText model\n",
    "# model = KeyedVectors.load_word2vec_format('path/to/fasttext.bin', binary=True)\n",
    "\n",
    "def get_synonyms_kv(word, k=5):\n",
    "    try:\n",
    "        # get the k most similar words to the given word from the FastText model\n",
    "        synonyms = vec_model.most_similar(word, topn=k)\n",
    "        return [synonym[0] for synonym in synonyms]\n",
    "    except KeyError:\n",
    "        # handle the case where the word is not in the FastText model\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_syn_dict_kv(words):\n",
    "    # Dictionary to hold the synonyms\n",
    "    synonyms_dict = {}\n",
    "\n",
    "    # Get the synonyms for each word and add them to the dictionary\n",
    "    for word in words:\n",
    "        synonyms = get_synonyms_kv(word)\n",
    "        synonyms_dict[word] = synonyms\n",
    "\n",
    "    # Print the dictionary\n",
    "    # print(synonyms_dict)\n",
    "    return synonyms_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syn_dict = get_syn_dict_kv(top_words)\n",
    "# syn_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sandbox\n",
    "import spacy\n",
    "import re\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# NER = ['ORG', 'PERSON', 'GPE', 'DATE', 'TIME', 'PRODUCT']\n",
    "# sent = \"Today is a Tuesday , tomorrow will be Wednesday. We will be going to Mercury Finance Co. on Thursday!\"\n",
    "# topkwords = {\"Tuesday\": [\"Thursday\", \"BLANK\"], \"be\": [\"is\", \"BLANK\"], \"Mercury Finance Co.\": [\"BLANK\"]} #{topk: [synonyms]}\n",
    "\n",
    "# orig_attrib_score = get_attribution_score(word_attributions)\n",
    "# orig_label = true_auth\n",
    "# new_label = \"\"\n",
    "# curr_sent = sent #the most recent sentence form; keep to revert back to after subbing a word\n",
    "# test_sent = \"\" #for testing out replacement words\n",
    "# new_sent = \"\" #what is returned with the best subs\n",
    "\n",
    "\n",
    "def sub_words(original_text,synonym_dict, original_label, original_attribution_score):\n",
    "    # doc = nlp(original_text) #the most recent sentence form; keep to revert back to after subbing a word\n",
    "    # doc_ents = [str(ent) for ent in doc.ents] #get the entities to skip over\n",
    "\n",
    "    curr_text = original_text\n",
    "    curr_text = re.sub(r'\\s+', ' ', curr_text)\n",
    "    \n",
    "    test_text = \"\" #for testing out replacement words\n",
    "    new_label = \"\"\n",
    "\n",
    "    for key in synonym_dict: #loop through top k words\n",
    "    #     if key in doc_ents: #if key is an entity, skip it\n",
    "    #         # print(\"NER\")\n",
    "    #         continue\n",
    "        if synonym_dict[key]:#make sure the list of synonyms for a key isn't empty\n",
    "            for value in synonym_dict[key]: #loop through list of values for each top k word\n",
    "                test_text = curr_text.replace(key, value) #replace keyword\n",
    "\n",
    "                word_attributions = cls_explainer(test_text) #reclassify with replacement\n",
    "                del word_attributions[0]\n",
    "                \n",
    "                new_attrib_score = get_attribution_score(word_attributions) #get new attribution score\n",
    "                new_label = cls_explainer.predicted_class_name\n",
    "\n",
    "                if new_label != original_label: #SUCCESS\n",
    "                    print(\"SUCCESS!\")\n",
    "                    curr_text = test_text\n",
    "                    return curr_text\n",
    "                \n",
    "                elif new_attrib_score < original_attribution_score:\n",
    "                    curr_text = test_text #updating the current sentence with the new word that caused the attribution score to drop\n",
    "                else: #reset sentence back to previous to try other values if the score doesnt drop or the label doesnt change\n",
    "                    test_text = curr_text \n",
    "        else:#go to the next key\n",
    "            continue\n",
    "    print(\"Unable to change label.\")\n",
    "    return curr_text#, new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = sub_words(text, syn_dict, cls_explainer.predicted_class_name, original_attribution_score)\n",
    "# sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_exp = cls_explainer(sentence[0])\n",
    "# cls_explainer.visualize(true_class=true_auth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test text --> explainer --> word attributions, predicted class name\n",
    "word attributions --> sort and select top k --> top k\n",
    "top k --> find top n synonyms for each k --> substitute & explain/predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "      <th>encoded-author</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1735</td>\n",
       "      <td>china on tuesday rolled out the memory of long...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2709</td>\n",
       "      <td>cisco systems inc fell more than two points in...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>251</td>\n",
       "      <td>caterpillar inc the worlds largest constructio...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4284</td>\n",
       "      <td>standard amp poors sampp on tuesday gave inves...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1027</td>\n",
       "      <td>a powerful car bomb exploded in the northwest ...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>3596</td>\n",
       "      <td>china showered the world trade organisation wt...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1432</td>\n",
       "      <td>western oil firms frustrated by moscows footdr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1898</td>\n",
       "      <td>hong kongs leaderinwaiting tung cheehwa on fri...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>3498</td>\n",
       "      <td>britains express newspapers seeking to halt a ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>4972</td>\n",
       "      <td>british defence to electronics giant general e...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                               Text   \n",
       "0          1735  china on tuesday rolled out the memory of long...  \\\n",
       "1          2709  cisco systems inc fell more than two points in...   \n",
       "2           251  caterpillar inc the worlds largest constructio...   \n",
       "3          4284  standard amp poors sampp on tuesday gave inves...   \n",
       "4          1027  a powerful car bomb exploded in the northwest ...   \n",
       "..          ...                                                ...   \n",
       "995        3596  china showered the world trade organisation wt...   \n",
       "996        1432  western oil firms frustrated by moscows footdr...   \n",
       "997        1898  hong kongs leaderinwaiting tung cheehwa on fri...   \n",
       "998        3498  britains express newspapers seeking to halt a ...   \n",
       "999        4972  british defence to electronics giant general e...   \n",
       "\n",
       "     encoded-author  pred  \n",
       "0                 4     4  \n",
       "1                 1     1  \n",
       "2                 3     3  \n",
       "3                 4     4  \n",
       "4                 0     5  \n",
       "..              ...   ...  \n",
       "995               1     1  \n",
       "996               0     0  \n",
       "997               4     4  \n",
       "998               5     5  \n",
       "999               5     5  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified_data = pd.read_csv(r'distilbert_finetuned_c50_w2v\\finetuned_distilbert_c50_w2v_pt25_testClassifications.csv')\n",
    "classified_data#= classified_data.rename(columns={'encoded-author': 'encoded-cluster'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>encoded-author</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>china on tuesday rolled out the memory of long...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cisco systems inc fell more than two points in...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>caterpillar inc the worlds largest constructio...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>standard amp poors sampp on tuesday gave inves...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a powerful car bomb exploded in the northwest ...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>china showered the world trade organisation wt...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>western oil firms frustrated by moscows footdr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>hong kongs leaderinwaiting tung cheehwa on fri...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>britains express newspapers seeking to halt a ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>british defence to electronics giant general e...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  encoded-author  pred\n",
       "0    china on tuesday rolled out the memory of long...               4     4\n",
       "1    cisco systems inc fell more than two points in...               1     1\n",
       "2    caterpillar inc the worlds largest constructio...               3     3\n",
       "3    standard amp poors sampp on tuesday gave inves...               4     4\n",
       "4    a powerful car bomb exploded in the northwest ...               0     5\n",
       "..                                                 ...             ...   ...\n",
       "995  china showered the world trade organisation wt...               1     1\n",
       "996  western oil firms frustrated by moscows footdr...               0     0\n",
       "997  hong kongs leaderinwaiting tung cheehwa on fri...               4     4\n",
       "998  britains express newspapers seeking to halt a ...               5     5\n",
       "999  british defence to electronics giant general e...               5     5\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified_data =classified_data.drop(classified_data.columns[0],axis=1)\n",
    "classified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>encoded-author</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a powerful car bomb exploded in the northwest ...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>british bank barclays tuesday sold its global ...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>christian salvesen moved to silence critics on...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>schroders plc one of the great british merchan...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cocoa bean exports from ivory coast jumped to ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>seven years after general motors corp created ...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>beleaguered auto lender mercury finance co sai...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>when thousands of auto dealers rolled into las...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>sun microsystems inc said monday it will move ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>britains nuclear generator british energy plc ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  encoded-author  pred\n",
       "0    a powerful car bomb exploded in the northwest ...               0     5\n",
       "1    british bank barclays tuesday sold its global ...               1     4\n",
       "2    christian salvesen moved to silence critics on...               5     2\n",
       "3    schroders plc one of the great british merchan...               3     0\n",
       "4    cocoa bean exports from ivory coast jumped to ...               4     1\n",
       "..                                                 ...             ...   ...\n",
       "143  seven years after general motors corp created ...               1     4\n",
       "144  beleaguered auto lender mercury finance co sai...               0     3\n",
       "145  when thousands of auto dealers rolled into las...               1     4\n",
       "146  sun microsystems inc said monday it will move ...               4     1\n",
       "147  britains nuclear generator british energy plc ...               3     1\n",
       "\n",
       "[148 rows x 3 columns]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctly_classified = classified_data[classified_data['encoded-author'] != classified_data['pred']]\n",
    "correctly_classified.drop(correctly_classified.columns[0],axis=1)\n",
    "correctly_classified = correctly_classified.reset_index(drop=True)\n",
    "correctly_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = correctly_classified['Text'].to_list()\n",
    "y_test = correctly_classified['encoded-author'].to_list()\n",
    "text = str(X_test[28])\n",
    "true_auth = y_test[28]\n",
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_labels = len(correctly_classified['encoded-author'].unique())#len(train_data['Author'].unique())\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased',  do_lower_case=True)\n",
    "\n",
    "# load the saved model\n",
    "saved_model_path = r\"distilbert_finetuned_c50_w2v\\distilbert_finetuned_w2v_25.pt\"\n",
    "saved_model = torch.load(saved_model_path, map_location=torch.device('cpu'))[\"model_state_dict\"]\n",
    "\n",
    "# get the shape of the current model's classifier layer\n",
    "current_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=N_labels)\n",
    "current_weight_shape = current_model.classifier.weight.shape\n",
    "current_bias_shape = current_model.classifier.bias.shape\n",
    "\n",
    "# modify the saved model's classifier layer\n",
    "saved_model[\"classifier.weight\"] = saved_model[\"classifier.weight\"][:current_weight_shape[0], :]\n",
    "saved_model[\"classifier.bias\"] = saved_model[\"classifier.bias\"][:current_bias_shape[0]]\n",
    "\n",
    "# load the modified saved model into the current model\n",
    "current_model.to(device)\n",
    "current_model.load_state_dict(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_explainer = SequenceClassificationExplainer(current_model, tokenizer)#, custom_labels=set(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: although a canadian federal election is not expected to be called until this weekend the mudslinging has already begun\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('[CLS]', 0.0),\n",
       " ('although', 0.16466716587022343),\n",
       " ('a', 0.3276135240589652),\n",
       " ('canadian', 0.2505077284974645),\n",
       " ('federal', 0.15497067175033694),\n",
       " ('election', 0.15669370051232598),\n",
       " ('is', 0.24785596228637974),\n",
       " ('not', 0.1762708935991498),\n",
       " ('expected', 0.11468274592209601),\n",
       " ('to', 0.22708135519372954),\n",
       " ('be', 0.15275231067569991),\n",
       " ('called', 0.26012392512293114),\n",
       " ('until', 0.38119367811449123),\n",
       " ('this', 0.30652626886531825),\n",
       " ('weekend', 0.03981159119581413),\n",
       " ('the', 0.20561479622806153),\n",
       " ('mud', 0.05337092448099583),\n",
       " ('##sling', 0.012339008779694845),\n",
       " ('##ing', 0.07203904787454071),\n",
       " ('has', 0.3289383542447527),\n",
       " ('already', 0.1703913812678424),\n",
       " ('begun', 0.27831273007095736),\n",
       " ('[SEP]', 0.0)]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_attributions = cls_explainer(text)\n",
    "# del word_attributions[0]\n",
    "print(f\"Text:{text}\")\n",
    "word_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: 0, Predicted: LABEL_0\n",
      "Text: air france is linking up with two major us carriers delta air lines and continental in a transatlantic alliance crucial to its ability to keep up with its major european rivals\n"
     ]
    }
   ],
   "source": [
    "print(f\"Actual: {true_auth}, Predicted: {cls_explainer.predicted_class_name}\")\n",
    "#predicted starts at 0 so +1\n",
    "print(f\"Text: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0</b></text></td><td><text style=\"padding-right:2em\"><b>4.96</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> air                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> france                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> linking                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> up                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> two                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> major                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> carriers                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> delta                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> air                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lines                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> continental                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> transatlantic                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> alliance                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> crucial                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> its                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ability                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> keep                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> up                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> its                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> major                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> european                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rivals                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0</b></text></td><td><text style=\"padding-right:2em\"><b>4.96</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> air                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> france                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> linking                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> up                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> two                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> major                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> carriers                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> delta                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> air                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lines                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> continental                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> transatlantic                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> alliance                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> crucial                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> its                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ability                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> keep                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> up                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> its                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> major                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> european                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rivals                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_explainer.visualize(true_class=true_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.542615141529567"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_attribution_score = get_attribution_score(word_attributions)\n",
    "original_attribution_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', -0.010615136881070705),\n",
       " ('television', 0.03221066216463168),\n",
       " ('industrys', 0.11658978595490384),\n",
       " ('new', 0.03024271820440124),\n",
       " ('ratings', 0.00790119365690451),\n",
       " ('system', 0.049031277488914574),\n",
       " ('is', 0.0805620182687031),\n",
       " ('a', 0.7092503324558732),\n",
       " ('bust', 0.09086851641083298),\n",
       " ('and', 0.07243187056596428),\n",
       " ('is', -0.05547024102542117),\n",
       " ('rife', -0.013960419118532835),\n",
       " ('with', 0.04251547239260561),\n",
       " ('inconsistencies', 0.04692966869367095),\n",
       " ('that', 0.03211317965527961),\n",
       " ('make', 0.03682832055236158),\n",
       " ('it', -0.004514299874056502),\n",
       " ('tougher', -0.09217123755915176),\n",
       " ('for', -0.0038818549190289308),\n",
       " ('parents', 0.10027275661494167),\n",
       " ('to', -0.035084959901587005),\n",
       " ('pick', 0.08342817317356782),\n",
       " ('shows', 0.02372020508680038),\n",
       " ('appropriate', 0.03250269176827331),\n",
       " ('for', -0.01040339298609321),\n",
       " ('kids', 0.11963611692238996),\n",
       " ('a', 0.5499101356608571),\n",
       " ('conservative', -0.054956152279288586),\n",
       " ('media', 0.009140603319187346),\n",
       " ('watchdog', -0.018517697354787352),\n",
       " ('group', 0.05260599875171374),\n",
       " ('charged', 0.10196092425307314),\n",
       " ('in', 0.13881067053847387),\n",
       " ('a', 0.25406798892717464),\n",
       " ('new', -0.035260937112837815),\n",
       " ('study', 0.021887306008039273),\n",
       " ('on', 0.005828979091035068),\n",
       " ('tuesday', 0.036203903960847494)]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_word_attributions = get_full_words(text, word_attributions)\n",
    "full_word_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['canadas',\n",
       "  'biggest',\n",
       "  'thirdquarter',\n",
       "  'buoyant',\n",
       "  'poised',\n",
       "  'setting',\n",
       "  'earnings',\n",
       "  'round'],\n",
       " 'canadas six biggest banks are poised for a further round of buoyant profits when they begin reporting thirdquarter earnings tuesday setting the stage for a third consecutive record year analysts said')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words = get_topk(text, full_word_attributions)\n",
    "top_words, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'canadas': ['Canada'],\n",
       " 'biggest': ['freehanded', 'crowing', 'swelled', 'enceinte', 'handsome'],\n",
       " 'thirdquarter': [],\n",
       " 'buoyant': ['perky', 'floaty', 'chirpy'],\n",
       " 'poised': ['brace',\n",
       "  'self-contained',\n",
       "  'self-collected',\n",
       "  'poise',\n",
       "  'self-possessed'],\n",
       " 'setting': ['rig', 'place', 'put', 'jell', 'coif'],\n",
       " 'earnings': ['earn', 'wage', 'garner', 'realize', 'pay'],\n",
       " 'round': ['around', 'pear-shaped', 'rung', 'rotund', 'cycle']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WordNet\n",
    "syn_dict = get_syn_dict_wn(top_words)\n",
    "# syn_dict\n",
    "\n",
    "# Gensim with fasttext model\n",
    "# syn_dict = get_syn_dict_kv(top_words)\n",
    "syn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to change label.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Canada six freehanded banks are brace for a further apear-shaped of perky profits when they begin reporting thirdquarter earn tuesday rig the stage for a third consecutive record year analysts said'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = sub_words(text, syn_dict, cls_explainer.predicted_class_name, original_attribution_score)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>6</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_6 (0.80)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_6</b></text></td><td><text style=\"padding-right:2em\"><b>0.94</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 54%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> canada                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> six                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bo                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ast                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ful                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> banks                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> are                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> po                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ise                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> further                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> aa                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##tta                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ck                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> drinks                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> out                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> golf                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> float                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##y                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> profits                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> when                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> they                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> begin                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> reporting                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> third                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##qua                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##rter                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> realise                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> tuesday                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dress                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> stage                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> third                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> consecutive                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> record                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> year                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> analysts                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> said                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>6</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_6 (0.80)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_6</b></text></td><td><text style=\"padding-right:2em\"><b>0.94</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 54%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> canada                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> six                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bo                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ast                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ful                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> banks                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> are                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> po                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ise                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> further                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> aa                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##tta                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ck                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> drinks                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> out                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> golf                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> float                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##y                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> profits                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> when                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> they                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> begin                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> reporting                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> third                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##qua                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##rter                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> realise                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> tuesday                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dress                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> stage                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> third                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> consecutive                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> record                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> year                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> analysts                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> said                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_exp = cls_explainer(sentence)\n",
    "cls_explainer.visualize(true_class=true_auth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through multiple samples\n",
    "results = pd.DataFrame(columns=['Original Label', 'Original Attribution Score', 'Original Text', 'New Label', 'New Attribution Score', 'New text', 'Misclassified'])\n",
    "\n",
    "def generate_adv(text):\n",
    "    cls_explainer = SequenceClassificationExplainer(current_model, tokenizer)#, custom_labels=set(y_test))\n",
    "    \n",
    "    word_attributions = cls_explainer(text)\n",
    "    del word_attributions[0]\n",
    "    original_label = cls_explainer.predicted_class_name\n",
    "    original_attribution_score = get_attribution_score(word_attributions)\n",
    "\n",
    "    full_word_attributions = get_full_words(text, word_attributions)\n",
    "\n",
    "    top_words = get_topk(text, full_word_attributions)\n",
    "    \n",
    "    # syn_dict = get_syn_dict_wn(top_words)\n",
    "    syn_dict = get_syn_dict_kv(top_words)\n",
    "\n",
    "    adv = sub_words(text, syn_dict, cls_explainer.predicted_class_name, original_attribution_score)\n",
    "    adv_attributions = cls_explainer(adv)\n",
    "    adv_label = cls_explainer.predicted_class_name\n",
    "    adv_attrib_score = get_attribution_score(adv_attributions) \n",
    "    if original_label != adv_label:\n",
    "        misclassified = True\n",
    "    else:\n",
    "        misclassified = False\n",
    "\n",
    "    return {'Original Label': original_label, 'Original Attribution Score': original_attribution_score,\n",
    "             'Original Text': text, 'New Label': adv_label, 'New Attribution Score': adv_attrib_score, \n",
    "             'New text': adv, 'Misclassified': misclassified}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "25\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "50\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "75\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "100\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "125\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "150\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "175\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "200\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "225\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "250\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "275\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "300\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "325\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "350\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "375\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "400\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "425\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "450\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "475\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "500\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "525\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "550\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "575\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "600\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "625\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "650\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "675\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "700\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "725\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "750\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "775\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "800\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "825\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "Unable to change label.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "850\n",
      "Unable to change label.\n",
      "Unable to change label.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Label</th>\n",
       "      <th>Original Attribution Score</th>\n",
       "      <th>Original Text</th>\n",
       "      <th>New Label</th>\n",
       "      <th>New Attribution Score</th>\n",
       "      <th>New text</th>\n",
       "      <th>Misclassified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LABEL_4</td>\n",
       "      <td>5.50</td>\n",
       "      <td>china on tuesday rolled out the memory of long...</td>\n",
       "      <td>LABEL_4</td>\n",
       "      <td>5.43</td>\n",
       "      <td>porcelain on tuesday rolling out the memories-...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LABEL_1</td>\n",
       "      <td>1.49</td>\n",
       "      <td>cisco systems inc fell more than two points in...</td>\n",
       "      <td>LABEL_1</td>\n",
       "      <td>0.88</td>\n",
       "      <td>Leuciscuses systems ltd. falling more than two...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LABEL_3</td>\n",
       "      <td>1.11</td>\n",
       "      <td>caterpillar inc the worlds largest constructio...</td>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>5.85</td>\n",
       "      <td>caterpillar-likes inc the worlds largest const...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LABEL_4</td>\n",
       "      <td>5.75</td>\n",
       "      <td>standard amp poors sampp on tuesday gave inves...</td>\n",
       "      <td>LABEL_4</td>\n",
       "      <td>5.73</td>\n",
       "      <td>standards amplifier poor- samplifierp on tuesd...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LABEL_1</td>\n",
       "      <td>1.36</td>\n",
       "      <td>sir ian prosser chairman of brewertoleisure gr...</td>\n",
       "      <td>LABEL_1</td>\n",
       "      <td>1.19</td>\n",
       "      <td>yesmisterree.s ean prosser vice-co-Chairman of...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>LABEL_1</td>\n",
       "      <td>1.37</td>\n",
       "      <td>china showered the world trade organisation wt...</td>\n",
       "      <td>LABEL_4</td>\n",
       "      <td>5.37</td>\n",
       "      <td>china showered the world--and trade organisati...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>5.39</td>\n",
       "      <td>western oil firms frustrated by moscows footdr...</td>\n",
       "      <td>LABEL_3</td>\n",
       "      <td>0.86</td>\n",
       "      <td>western oil companies frustrated by moscows dr...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>LABEL_4</td>\n",
       "      <td>5.82</td>\n",
       "      <td>hong kongs leaderinwaiting tung cheehwa on fri...</td>\n",
       "      <td>LABEL_1</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>hong kongs leaderinwaiting chong cheehwa on fr...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>LABEL_5</td>\n",
       "      <td>4.84</td>\n",
       "      <td>britains express newspapers seeking to halt a ...</td>\n",
       "      <td>LABEL_5</td>\n",
       "      <td>4.80</td>\n",
       "      <td>englands re-expressing newspaper seek to stops...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>LABEL_5</td>\n",
       "      <td>5.01</td>\n",
       "      <td>british defence to electronics giant general e...</td>\n",
       "      <td>LABEL_5</td>\n",
       "      <td>4.94</td>\n",
       "      <td>britsh defense to opto-electronics gigantic no...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>852 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Original Label  Original Attribution Score   \n",
       "0          LABEL_4                        5.50  \\\n",
       "1          LABEL_1                        1.49   \n",
       "2          LABEL_3                        1.11   \n",
       "3          LABEL_4                        5.75   \n",
       "4          LABEL_1                        1.36   \n",
       "..             ...                         ...   \n",
       "847        LABEL_1                        1.37   \n",
       "848        LABEL_0                        5.39   \n",
       "849        LABEL_4                        5.82   \n",
       "850        LABEL_5                        4.84   \n",
       "851        LABEL_5                        5.01   \n",
       "\n",
       "                                         Original Text New Label   \n",
       "0    china on tuesday rolled out the memory of long...   LABEL_4  \\\n",
       "1    cisco systems inc fell more than two points in...   LABEL_1   \n",
       "2    caterpillar inc the worlds largest constructio...   LABEL_0   \n",
       "3    standard amp poors sampp on tuesday gave inves...   LABEL_4   \n",
       "4    sir ian prosser chairman of brewertoleisure gr...   LABEL_1   \n",
       "..                                                 ...       ...   \n",
       "847  china showered the world trade organisation wt...   LABEL_4   \n",
       "848  western oil firms frustrated by moscows footdr...   LABEL_3   \n",
       "849  hong kongs leaderinwaiting tung cheehwa on fri...   LABEL_1   \n",
       "850  britains express newspapers seeking to halt a ...   LABEL_5   \n",
       "851  british defence to electronics giant general e...   LABEL_5   \n",
       "\n",
       "     New Attribution Score                                           New text   \n",
       "0                     5.43  porcelain on tuesday rolling out the memories-...  \\\n",
       "1                     0.88  Leuciscuses systems ltd. falling more than two...   \n",
       "2                     5.85  caterpillar-likes inc the worlds largest const...   \n",
       "3                     5.73  standards amplifier poor- samplifierp on tuesd...   \n",
       "4                     1.19  yesmisterree.s ean prosser vice-co-Chairman of...   \n",
       "..                     ...                                                ...   \n",
       "847                   5.37  china showered the world--and trade organisati...   \n",
       "848                   0.86  western oil companies frustrated by moscows dr...   \n",
       "849                  -0.04  hong kongs leaderinwaiting chong cheehwa on fr...   \n",
       "850                   4.80  englands re-expressing newspaper seek to stops...   \n",
       "851                   4.94  britsh defense to opto-electronics gigantic no...   \n",
       "\n",
       "     Misclassified  \n",
       "0            False  \n",
       "1            False  \n",
       "2             True  \n",
       "3            False  \n",
       "4            False  \n",
       "..             ...  \n",
       "847           True  \n",
       "848           True  \n",
       "849           True  \n",
       "850          False  \n",
       "851          False  \n",
       "\n",
       "[852 rows x 7 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for text in X_test:\n",
    "    row = generate_adv(text)\n",
    "    # print(row)\n",
    "    results.loc[len(results)] = row\n",
    "    if len(results) % 25 == 0:\n",
    "        print(len(results))\n",
    "    # results.append(row, ignore_index=True)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "567"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv = results[results['Misclassified'] == False]\n",
    "len(adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(r'distilbert_finetuned_c50_w2v\\c50_w2v_adversarialGenerations_kv.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity \n",
    "\n",
    " The similarity scores will range from -1 to 1, where -1 indicates completely dissimilar text and 1 indicates identical text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "sim_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "sim_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def encode_text(text):\n",
    "    tokens = sim_tokenizer.encode(text, add_special_tokens=True)\n",
    "    input_ids = torch.tensor(tokens).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = sim_model(input_ids)\n",
    "        embeddings = outputs[0][:, 0, :].numpy()\n",
    "    return embeddings\n",
    "\n",
    "df = pd.read_csv(r'distilbert_finetuned_c50_w2v\\c50_w2v_adversarialGenerations_kv.csv')\n",
    "embeddings1 = df['Original Text'].apply(lambda x: encode_text(x)).to_numpy()\n",
    "embeddings2 = df['New text'].apply(lambda x: encode_text(x)).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    similarity_scores.append(cosine_similarity(embeddings1[i].reshape(1, -1), embeddings2[i].reshape(1, -1))[0][0])\n",
    "df['similarity'] = similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Original Label</th>\n",
       "      <th>Original Attribution Score</th>\n",
       "      <th>Original Text</th>\n",
       "      <th>New Label</th>\n",
       "      <th>New Attribution Score</th>\n",
       "      <th>New text</th>\n",
       "      <th>Misclassified</th>\n",
       "      <th>bleu_score</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>LABEL_4</td>\n",
       "      <td>5.50</td>\n",
       "      <td>china on tuesday rolled out the memory of long...</td>\n",
       "      <td>LABEL_4</td>\n",
       "      <td>5.43</td>\n",
       "      <td>porcelain on tuesday rolling out the memories-...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>LABEL_1</td>\n",
       "      <td>1.49</td>\n",
       "      <td>cisco systems inc fell more than two points in...</td>\n",
       "      <td>LABEL_1</td>\n",
       "      <td>0.88</td>\n",
       "      <td>Leuciscuses systems ltd. falling more than two...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>LABEL_3</td>\n",
       "      <td>1.11</td>\n",
       "      <td>caterpillar inc the worlds largest constructio...</td>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>5.85</td>\n",
       "      <td>caterpillar-likes inc the worlds largest const...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>LABEL_4</td>\n",
       "      <td>5.75</td>\n",
       "      <td>standard amp poors sampp on tuesday gave inves...</td>\n",
       "      <td>LABEL_4</td>\n",
       "      <td>5.73</td>\n",
       "      <td>standards amplifier poor- samplifierp on tuesd...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>LABEL_1</td>\n",
       "      <td>1.36</td>\n",
       "      <td>sir ian prosser chairman of brewertoleisure gr...</td>\n",
       "      <td>LABEL_1</td>\n",
       "      <td>1.19</td>\n",
       "      <td>yesmisterree.s ean prosser vice-co-Chairman of...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>847</td>\n",
       "      <td>847</td>\n",
       "      <td>LABEL_1</td>\n",
       "      <td>1.37</td>\n",
       "      <td>china showered the world trade organisation wt...</td>\n",
       "      <td>LABEL_4</td>\n",
       "      <td>5.37</td>\n",
       "      <td>china showered the world--and trade organisati...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>848</td>\n",
       "      <td>848</td>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>5.39</td>\n",
       "      <td>western oil firms frustrated by moscows footdr...</td>\n",
       "      <td>LABEL_3</td>\n",
       "      <td>0.86</td>\n",
       "      <td>western oil companies frustrated by moscows dr...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>849</td>\n",
       "      <td>849</td>\n",
       "      <td>LABEL_4</td>\n",
       "      <td>5.82</td>\n",
       "      <td>hong kongs leaderinwaiting tung cheehwa on fri...</td>\n",
       "      <td>LABEL_1</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>hong kongs leaderinwaiting chong cheehwa on fr...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>850</td>\n",
       "      <td>850</td>\n",
       "      <td>LABEL_5</td>\n",
       "      <td>4.84</td>\n",
       "      <td>britains express newspapers seeking to halt a ...</td>\n",
       "      <td>LABEL_5</td>\n",
       "      <td>4.80</td>\n",
       "      <td>englands re-expressing newspaper seek to stops...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>851</td>\n",
       "      <td>851</td>\n",
       "      <td>LABEL_5</td>\n",
       "      <td>5.01</td>\n",
       "      <td>british defence to electronics giant general e...</td>\n",
       "      <td>LABEL_5</td>\n",
       "      <td>4.94</td>\n",
       "      <td>britsh defense to opto-electronics gigantic no...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>852 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.1  Unnamed: 0 Original Label  Original Attribution Score   \n",
       "0               0           0        LABEL_4                        5.50  \\\n",
       "1               1           1        LABEL_1                        1.49   \n",
       "2               2           2        LABEL_3                        1.11   \n",
       "3               3           3        LABEL_4                        5.75   \n",
       "4               4           4        LABEL_1                        1.36   \n",
       "..            ...         ...            ...                         ...   \n",
       "847           847         847        LABEL_1                        1.37   \n",
       "848           848         848        LABEL_0                        5.39   \n",
       "849           849         849        LABEL_4                        5.82   \n",
       "850           850         850        LABEL_5                        4.84   \n",
       "851           851         851        LABEL_5                        5.01   \n",
       "\n",
       "                                         Original Text New Label   \n",
       "0    china on tuesday rolled out the memory of long...   LABEL_4  \\\n",
       "1    cisco systems inc fell more than two points in...   LABEL_1   \n",
       "2    caterpillar inc the worlds largest constructio...   LABEL_0   \n",
       "3    standard amp poors sampp on tuesday gave inves...   LABEL_4   \n",
       "4    sir ian prosser chairman of brewertoleisure gr...   LABEL_1   \n",
       "..                                                 ...       ...   \n",
       "847  china showered the world trade organisation wt...   LABEL_4   \n",
       "848  western oil firms frustrated by moscows footdr...   LABEL_3   \n",
       "849  hong kongs leaderinwaiting tung cheehwa on fri...   LABEL_1   \n",
       "850  britains express newspapers seeking to halt a ...   LABEL_5   \n",
       "851  british defence to electronics giant general e...   LABEL_5   \n",
       "\n",
       "     New Attribution Score                                           New text   \n",
       "0                     5.43  porcelain on tuesday rolling out the memories-...  \\\n",
       "1                     0.88  Leuciscuses systems ltd. falling more than two...   \n",
       "2                     5.85  caterpillar-likes inc the worlds largest const...   \n",
       "3                     5.73  standards amplifier poor- samplifierp on tuesd...   \n",
       "4                     1.19  yesmisterree.s ean prosser vice-co-Chairman of...   \n",
       "..                     ...                                                ...   \n",
       "847                   5.37  china showered the world--and trade organisati...   \n",
       "848                   0.86  western oil companies frustrated by moscows dr...   \n",
       "849                  -0.04  hong kongs leaderinwaiting chong cheehwa on fr...   \n",
       "850                   4.80  englands re-expressing newspaper seek to stops...   \n",
       "851                   4.94  britsh defense to opto-electronics gigantic no...   \n",
       "\n",
       "     Misclassified  bleu_score  similarity  \n",
       "0            False        0.18        0.95  \n",
       "1            False        0.31        0.96  \n",
       "2             True        0.86        0.99  \n",
       "3            False        0.16        0.96  \n",
       "4            False        0.37        0.98  \n",
       "..             ...         ...         ...  \n",
       "847           True        0.91        0.99  \n",
       "848           True        0.22        0.96  \n",
       "849           True        0.68        0.99  \n",
       "850          False        0.23        0.93  \n",
       "851          False        0.17        0.92  \n",
       "\n",
       "[852 rows x 11 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv(r'distilbert_finetuned_c50_w2v\\c50_w2v_adversarialGenerations_kv.csv')\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU Score\n",
    "\n",
    "as a general rule of thumb, higher BLEU scores are typically better, with scores above 0.4 or 0.5 often considered to be strong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Original Label</th>\n",
       "      <th>Original Attribution Score</th>\n",
       "      <th>Original Text</th>\n",
       "      <th>New Label</th>\n",
       "      <th>New Attribution Score</th>\n",
       "      <th>New text</th>\n",
       "      <th>Misclassified</th>\n",
       "      <th>similarity</th>\n",
       "      <th>bleu_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>LABEL_17</td>\n",
       "      <td>2.51</td>\n",
       "      <td>royal bank of scotland which reports full year...</td>\n",
       "      <td>LABEL_8</td>\n",
       "      <td>2.14</td>\n",
       "      <td>majestic cant building of scotland which repor...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>LABEL_28</td>\n",
       "      <td>1.06</td>\n",
       "      <td>russia is quietly importing some western grain...</td>\n",
       "      <td>LABEL_28</td>\n",
       "      <td>0.95</td>\n",
       "      <td>russia is quietly import some western caryopsi...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>LABEL_4</td>\n",
       "      <td>2.13</td>\n",
       "      <td>an official inquiry into australias financial ...</td>\n",
       "      <td>LABEL_4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>an prescribed interrogation into Australia fis...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>LABEL_26</td>\n",
       "      <td>1.36</td>\n",
       "      <td>toronto stocks ended softer on monday after th...</td>\n",
       "      <td>LABEL_6</td>\n",
       "      <td>1.24</td>\n",
       "      <td>toronto neckcloth ended softer on monday after...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>LABEL_6</td>\n",
       "      <td>1.43</td>\n",
       "      <td>canadas six biggest banks are poised for a fur...</td>\n",
       "      <td>LABEL_6</td>\n",
       "      <td>0.95</td>\n",
       "      <td>Canada six freehanded banks are brace for a fu...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>705</td>\n",
       "      <td>705</td>\n",
       "      <td>LABEL_39</td>\n",
       "      <td>3.69</td>\n",
       "      <td>conrail inc and csx corp so far have convinced...</td>\n",
       "      <td>LABEL_39</td>\n",
       "      <td>2.55</td>\n",
       "      <td>conrail Iraqi National Congress and csx corpor...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>706</td>\n",
       "      <td>706</td>\n",
       "      <td>LABEL_12</td>\n",
       "      <td>2.08</td>\n",
       "      <td>statistics canada has admitted to making a sig...</td>\n",
       "      <td>LABEL_6</td>\n",
       "      <td>2.15</td>\n",
       "      <td>statistics canada has intromit to earn a signi...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>707</td>\n",
       "      <td>707</td>\n",
       "      <td>LABEL_44</td>\n",
       "      <td>2.56</td>\n",
       "      <td>scottish amicable on monday confirmed that thr...</td>\n",
       "      <td>LABEL_44</td>\n",
       "      <td>1.87</td>\n",
       "      <td>scottish amicable on monday sustain that three...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>708</td>\n",
       "      <td>708</td>\n",
       "      <td>LABEL_5</td>\n",
       "      <td>2.59</td>\n",
       "      <td>mercury finance co said wednesday it found pro...</td>\n",
       "      <td>LABEL_31</td>\n",
       "      <td>0.83</td>\n",
       "      <td>Hg finance atomic number 27balt said wednesday...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>709</td>\n",
       "      <td>709</td>\n",
       "      <td>LABEL_19</td>\n",
       "      <td>2.68</td>\n",
       "      <td>shareholders in tiny british healthcare group ...</td>\n",
       "      <td>LABEL_19</td>\n",
       "      <td>1.49</td>\n",
       "      <td>shareholder in midget british health care math...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>710 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.1  Unnamed: 0 Original Label  Original Attribution Score   \n",
       "0               0           0       LABEL_17                        2.51  \\\n",
       "1               1           1       LABEL_28                        1.06   \n",
       "2               2           2        LABEL_4                        2.13   \n",
       "3               3           3       LABEL_26                        1.36   \n",
       "4               4           4        LABEL_6                        1.43   \n",
       "..            ...         ...            ...                         ...   \n",
       "705           705         705       LABEL_39                        3.69   \n",
       "706           706         706       LABEL_12                        2.08   \n",
       "707           707         707       LABEL_44                        2.56   \n",
       "708           708         708        LABEL_5                        2.59   \n",
       "709           709         709       LABEL_19                        2.68   \n",
       "\n",
       "                                         Original Text New Label   \n",
       "0    royal bank of scotland which reports full year...   LABEL_8  \\\n",
       "1    russia is quietly importing some western grain...  LABEL_28   \n",
       "2    an official inquiry into australias financial ...   LABEL_4   \n",
       "3    toronto stocks ended softer on monday after th...   LABEL_6   \n",
       "4    canadas six biggest banks are poised for a fur...   LABEL_6   \n",
       "..                                                 ...       ...   \n",
       "705  conrail inc and csx corp so far have convinced...  LABEL_39   \n",
       "706  statistics canada has admitted to making a sig...   LABEL_6   \n",
       "707  scottish amicable on monday confirmed that thr...  LABEL_44   \n",
       "708  mercury finance co said wednesday it found pro...  LABEL_31   \n",
       "709  shareholders in tiny british healthcare group ...  LABEL_19   \n",
       "\n",
       "     New Attribution Score                                           New text   \n",
       "0                     2.14  majestic cant building of scotland which repor...  \\\n",
       "1                     0.95  russia is quietly import some western caryopsi...   \n",
       "2                     1.00  an prescribed interrogation into Australia fis...   \n",
       "3                     1.24  toronto neckcloth ended softer on monday after...   \n",
       "4                     0.95  Canada six freehanded banks are brace for a fu...   \n",
       "..                     ...                                                ...   \n",
       "705                   2.55  conrail Iraqi National Congress and csx corpor...   \n",
       "706                   2.15  statistics canada has intromit to earn a signi...   \n",
       "707                   1.87  scottish amicable on monday sustain that three...   \n",
       "708                   0.83  Hg finance atomic number 27balt said wednesday...   \n",
       "709                   1.49  shareholder in midget british health care math...   \n",
       "\n",
       "     Misclassified  similarity  bleu_score  \n",
       "0             True        0.98        0.92  \n",
       "1            False        0.97        0.39  \n",
       "2            False        0.89        0.12  \n",
       "3             True        0.98        0.90  \n",
       "4            False        0.98        0.51  \n",
       "..             ...         ...         ...  \n",
       "705          False        0.96        0.00  \n",
       "706           True        0.95        0.25  \n",
       "707          False        0.91        0.25  \n",
       "708           True        0.97        0.57  \n",
       "709          False        0.99        0.25  \n",
       "\n",
       "[710 rows x 11 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "df = pd.read_csv(r'distilbert_finetuned_c50\\c50_adversarialGenerations_wn.csv')\n",
    "\n",
    "# define a function to calculate the BLEU score\n",
    "def calculate_bleu_score(reference, candidate):\n",
    "    return sentence_bleu([reference], candidate)\n",
    "\n",
    "# apply the function to each row of the DataFrame and store the result in a new column\n",
    "df['bleu_score'] = df.apply(lambda row: calculate_bleu_score(row['Original Text'].split(), row['New text'].split()), axis=1)\n",
    "\n",
    "df.to_csv(r'distilbert_finetuned_c50\\c50_adversarialGenerations_wn.csv')\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Clustered Adv On Author Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boeing co is expected to use the biennial farnborough air show in england next week to formally launch the longawaited stretch version of its 747 jumbo jet industry analysts say'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(r'distilbert_finetuned_c50_tfidf\\finetuned_distilbert_c50_tfidf_pt25_testClassifications.csv')\n",
    "a =test_df['Text'][0]\n",
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>the internet may be overflowing with new techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>the us postal service announced wednesday a pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>elementary school students with access to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>an influential internet organisation has backe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>an influential internet organisation has backe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>candidate00018</td>\n",
       "      <td>britains big banks look set to raise profits b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>candidate00047</td>\n",
       "      <td>after two years of hype and euphoria about the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>candidate00002</td>\n",
       "      <td>czech annual average consumer inflation eased ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>candidate00037</td>\n",
       "      <td>kellogg co whose profits for 1996 are under pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>candidate00018</td>\n",
       "      <td>londonbased international bank hsbc holdings p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Author                                               Text\n",
       "0     candidate00001  the internet may be overflowing with new techn...\n",
       "1     candidate00001  the us postal service announced wednesday a pl...\n",
       "2     candidate00001  elementary school students with access to the ...\n",
       "3     candidate00001  an influential internet organisation has backe...\n",
       "4     candidate00001  an influential internet organisation has backe...\n",
       "...              ...                                                ...\n",
       "2495  candidate00018  britains big banks look set to raise profits b...\n",
       "2496  candidate00047  after two years of hype and euphoria about the...\n",
       "2497  candidate00002  czech annual average consumer inflation eased ...\n",
       "2498  candidate00037  kellogg co whose profits for 1996 are under pr...\n",
       "2499  candidate00018  londonbased international bank hsbc holdings p...\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['candidate00032',\n",
       " 'candidate00022',\n",
       " 'candidate00010',\n",
       " 'candidate00003',\n",
       " 'candidate00008',\n",
       " 'candidate00038',\n",
       " 'candidate00049',\n",
       " 'candidate00037',\n",
       " 'candidate00036',\n",
       " 'candidate00011',\n",
       " 'candidate00002',\n",
       " 'candidate00047',\n",
       " 'candidate00004',\n",
       " 'candidate00030',\n",
       " 'candidate00013',\n",
       " 'candidate00019',\n",
       " 'candidate00004',\n",
       " 'candidate00007',\n",
       " 'candidate00044',\n",
       " 'candidate00035',\n",
       " 'candidate00023',\n",
       " 'candidate00045',\n",
       " 'candidate00006',\n",
       " 'candidate00014',\n",
       " 'candidate00041',\n",
       " 'candidate00022',\n",
       " 'candidate00041',\n",
       " 'candidate00041',\n",
       " 'candidate00049',\n",
       " 'candidate00038',\n",
       " 'candidate00008',\n",
       " 'candidate00001',\n",
       " 'candidate00041',\n",
       " 'candidate00048',\n",
       " 'candidate00001',\n",
       " 'candidate00012',\n",
       " 'candidate00031',\n",
       " 'candidate00042',\n",
       " 'candidate00047',\n",
       " 'candidate00046',\n",
       " 'candidate00001',\n",
       " 'candidate00036',\n",
       " 'candidate00022',\n",
       " 'candidate00011',\n",
       " 'candidate00019',\n",
       " 'candidate00004',\n",
       " 'candidate00022',\n",
       " 'candidate00030',\n",
       " 'candidate00015',\n",
       " 'candidate00035',\n",
       " 'candidate00004',\n",
       " 'candidate00002',\n",
       " 'candidate00031',\n",
       " 'candidate00030',\n",
       " 'candidate00038',\n",
       " 'candidate00008',\n",
       " 'candidate00018',\n",
       " 'candidate00019',\n",
       " 'candidate00007',\n",
       " 'candidate00032',\n",
       " 'candidate00045',\n",
       " 'candidate00044',\n",
       " 'candidate00045',\n",
       " 'candidate00026',\n",
       " 'candidate00021',\n",
       " 'candidate00009',\n",
       " 'candidate00032',\n",
       " 'candidate00034',\n",
       " 'candidate00025',\n",
       " 'candidate00026',\n",
       " 'candidate00029',\n",
       " 'candidate00050',\n",
       " 'candidate00028',\n",
       " 'candidate00023',\n",
       " 'candidate00030',\n",
       " 'candidate00011',\n",
       " 'candidate00045',\n",
       " 'candidate00048',\n",
       " 'candidate00050',\n",
       " 'candidate00028',\n",
       " 'candidate00034',\n",
       " 'candidate00027',\n",
       " 'candidate00034',\n",
       " 'candidate00039',\n",
       " 'candidate00021',\n",
       " 'candidate00032',\n",
       " 'candidate00040',\n",
       " 'candidate00048',\n",
       " 'candidate00019',\n",
       " 'candidate00041',\n",
       " 'candidate00012',\n",
       " 'candidate00045',\n",
       " 'candidate00045',\n",
       " 'candidate00004',\n",
       " 'candidate00029',\n",
       " 'candidate00027',\n",
       " 'candidate00036',\n",
       " 'candidate00020',\n",
       " 'candidate00048',\n",
       " 'candidate00035',\n",
       " 'candidate00034',\n",
       " 'candidate00001',\n",
       " 'candidate00029',\n",
       " 'candidate00002',\n",
       " 'candidate00023',\n",
       " 'candidate00008',\n",
       " 'candidate00024',\n",
       " 'candidate00024',\n",
       " 'candidate00007',\n",
       " 'candidate00006',\n",
       " 'candidate00017',\n",
       " 'candidate00020',\n",
       " 'candidate00050',\n",
       " 'candidate00019',\n",
       " 'candidate00025',\n",
       " 'candidate00046',\n",
       " 'candidate00030',\n",
       " 'candidate00010',\n",
       " 'candidate00039',\n",
       " 'candidate00021',\n",
       " 'candidate00049',\n",
       " 'candidate00005',\n",
       " 'candidate00013',\n",
       " 'candidate00027',\n",
       " 'candidate00046',\n",
       " 'candidate00012',\n",
       " 'candidate00006',\n",
       " 'candidate00044',\n",
       " 'candidate00021',\n",
       " 'candidate00038',\n",
       " 'candidate00013',\n",
       " 'candidate00007',\n",
       " 'candidate00002',\n",
       " 'candidate00007',\n",
       " 'candidate00029',\n",
       " 'candidate00036',\n",
       " 'candidate00048',\n",
       " 'candidate00001',\n",
       " 'candidate00021',\n",
       " 'candidate00017',\n",
       " 'candidate00013',\n",
       " 'candidate00024',\n",
       " 'candidate00007',\n",
       " 'candidate00025',\n",
       " 'candidate00049',\n",
       " 'candidate00022',\n",
       " 'candidate00033',\n",
       " 'candidate00023',\n",
       " 'candidate00019',\n",
       " 'candidate00025',\n",
       " 'candidate00034',\n",
       " 'candidate00016',\n",
       " 'candidate00007',\n",
       " 'candidate00038',\n",
       " 'candidate00031',\n",
       " 'candidate00018',\n",
       " 'candidate00016',\n",
       " 'candidate00042',\n",
       " 'candidate00024',\n",
       " 'candidate00010',\n",
       " 'candidate00026',\n",
       " 'candidate00011',\n",
       " 'candidate00040',\n",
       " 'candidate00011',\n",
       " 'candidate00014',\n",
       " 'candidate00011',\n",
       " 'candidate00027',\n",
       " 'candidate00031',\n",
       " 'candidate00013',\n",
       " 'candidate00043',\n",
       " 'candidate00018',\n",
       " 'candidate00040',\n",
       " 'candidate00007',\n",
       " 'candidate00018',\n",
       " 'candidate00044',\n",
       " 'candidate00021',\n",
       " 'candidate00036',\n",
       " 'candidate00023',\n",
       " 'candidate00042',\n",
       " 'candidate00048',\n",
       " 'candidate00023',\n",
       " 'candidate00038',\n",
       " 'candidate00035',\n",
       " 'candidate00012',\n",
       " 'candidate00034',\n",
       " 'candidate00032',\n",
       " 'candidate00027',\n",
       " 'candidate00042',\n",
       " 'candidate00048',\n",
       " 'candidate00011',\n",
       " 'candidate00010',\n",
       " 'candidate00011',\n",
       " 'candidate00014',\n",
       " 'candidate00036',\n",
       " 'candidate00029',\n",
       " 'candidate00033',\n",
       " 'candidate00042',\n",
       " 'candidate00030',\n",
       " 'candidate00001',\n",
       " 'candidate00048',\n",
       " 'candidate00023',\n",
       " 'candidate00042',\n",
       " 'candidate00026',\n",
       " 'candidate00028',\n",
       " 'candidate00044',\n",
       " 'candidate00013',\n",
       " 'candidate00034',\n",
       " 'candidate00050',\n",
       " 'candidate00010',\n",
       " 'candidate00041',\n",
       " 'candidate00011',\n",
       " 'candidate00032',\n",
       " 'candidate00010',\n",
       " 'candidate00002',\n",
       " 'candidate00039',\n",
       " 'candidate00044',\n",
       " 'candidate00042',\n",
       " 'candidate00019',\n",
       " 'candidate00020',\n",
       " 'candidate00026',\n",
       " 'candidate00028',\n",
       " 'candidate00041',\n",
       " 'candidate00013',\n",
       " 'candidate00032',\n",
       " 'candidate00008',\n",
       " 'candidate00020',\n",
       " 'candidate00023',\n",
       " 'candidate00005',\n",
       " 'candidate00011',\n",
       " 'candidate00040',\n",
       " 'candidate00037',\n",
       " 'candidate00010',\n",
       " 'candidate00025',\n",
       " 'candidate00041',\n",
       " 'candidate00043',\n",
       " 'candidate00030',\n",
       " 'candidate00014',\n",
       " 'candidate00003',\n",
       " 'candidate00016',\n",
       " 'candidate00018',\n",
       " 'candidate00045',\n",
       " 'candidate00036',\n",
       " 'candidate00038',\n",
       " 'candidate00032',\n",
       " 'candidate00034',\n",
       " 'candidate00025',\n",
       " 'candidate00009',\n",
       " 'candidate00010',\n",
       " 'candidate00003',\n",
       " 'candidate00045',\n",
       " 'candidate00015',\n",
       " 'candidate00002',\n",
       " 'candidate00033',\n",
       " 'candidate00007',\n",
       " 'candidate00002',\n",
       " 'candidate00029',\n",
       " 'candidate00001',\n",
       " 'candidate00007',\n",
       " 'candidate00032',\n",
       " 'candidate00037',\n",
       " 'candidate00048',\n",
       " 'candidate00044',\n",
       " 'candidate00046',\n",
       " 'candidate00038',\n",
       " 'candidate00050',\n",
       " 'candidate00026',\n",
       " 'candidate00041',\n",
       " 'candidate00014',\n",
       " 'candidate00026',\n",
       " 'candidate00002',\n",
       " 'candidate00015',\n",
       " 'candidate00017',\n",
       " 'candidate00041',\n",
       " 'candidate00033',\n",
       " 'candidate00026',\n",
       " 'candidate00016',\n",
       " 'candidate00006',\n",
       " 'candidate00002',\n",
       " 'candidate00023',\n",
       " 'candidate00001',\n",
       " 'candidate00016',\n",
       " 'candidate00045',\n",
       " 'candidate00008',\n",
       " 'candidate00045',\n",
       " 'candidate00015',\n",
       " 'candidate00036',\n",
       " 'candidate00016',\n",
       " 'candidate00001',\n",
       " 'candidate00044',\n",
       " 'candidate00029',\n",
       " 'candidate00003',\n",
       " 'candidate00017',\n",
       " 'candidate00015',\n",
       " 'candidate00018',\n",
       " 'candidate00030',\n",
       " 'candidate00043',\n",
       " 'candidate00004',\n",
       " 'candidate00005',\n",
       " 'candidate00024',\n",
       " 'candidate00038',\n",
       " 'candidate00045',\n",
       " 'candidate00036',\n",
       " 'candidate00038',\n",
       " 'candidate00005',\n",
       " 'candidate00004',\n",
       " 'candidate00044',\n",
       " 'candidate00033',\n",
       " 'candidate00049',\n",
       " 'candidate00021',\n",
       " 'candidate00036',\n",
       " 'candidate00013',\n",
       " 'candidate00005',\n",
       " 'candidate00042',\n",
       " 'candidate00019',\n",
       " 'candidate00031',\n",
       " 'candidate00043',\n",
       " 'candidate00044',\n",
       " 'candidate00036',\n",
       " 'candidate00002',\n",
       " 'candidate00024',\n",
       " 'candidate00035',\n",
       " 'candidate00040',\n",
       " 'candidate00047',\n",
       " 'candidate00030',\n",
       " 'candidate00049',\n",
       " 'candidate00035',\n",
       " 'candidate00038',\n",
       " 'candidate00004',\n",
       " 'candidate00013',\n",
       " 'candidate00040',\n",
       " 'candidate00015',\n",
       " 'candidate00036',\n",
       " 'candidate00022',\n",
       " 'candidate00034',\n",
       " 'candidate00037',\n",
       " 'candidate00021',\n",
       " 'candidate00046',\n",
       " 'candidate00024',\n",
       " 'candidate00038',\n",
       " 'candidate00030',\n",
       " 'candidate00003',\n",
       " 'candidate00043',\n",
       " 'candidate00027',\n",
       " 'candidate00035',\n",
       " 'candidate00025',\n",
       " 'candidate00006',\n",
       " 'candidate00037',\n",
       " 'candidate00043',\n",
       " 'candidate00038',\n",
       " 'candidate00011',\n",
       " 'candidate00012',\n",
       " 'candidate00049',\n",
       " 'candidate00002',\n",
       " 'candidate00042',\n",
       " 'candidate00036',\n",
       " 'candidate00033',\n",
       " 'candidate00032',\n",
       " 'candidate00030',\n",
       " 'candidate00047',\n",
       " 'candidate00032',\n",
       " 'candidate00006',\n",
       " 'candidate00014',\n",
       " 'candidate00034',\n",
       " 'candidate00044',\n",
       " 'candidate00023',\n",
       " 'candidate00019',\n",
       " 'candidate00041',\n",
       " 'candidate00015',\n",
       " 'candidate00022',\n",
       " 'candidate00047',\n",
       " 'candidate00008',\n",
       " 'candidate00017',\n",
       " 'candidate00036',\n",
       " 'candidate00032',\n",
       " 'candidate00003',\n",
       " 'candidate00047',\n",
       " 'candidate00018',\n",
       " 'candidate00043',\n",
       " 'candidate00007',\n",
       " 'candidate00014',\n",
       " 'candidate00045',\n",
       " 'candidate00007',\n",
       " 'candidate00010',\n",
       " 'candidate00004',\n",
       " 'candidate00047',\n",
       " 'candidate00045',\n",
       " 'candidate00036',\n",
       " 'candidate00001',\n",
       " 'candidate00016',\n",
       " 'candidate00047',\n",
       " 'candidate00025',\n",
       " 'candidate00037',\n",
       " 'candidate00032',\n",
       " 'candidate00017',\n",
       " 'candidate00021',\n",
       " 'candidate00028',\n",
       " 'candidate00034',\n",
       " 'candidate00046',\n",
       " 'candidate00007',\n",
       " 'candidate00001',\n",
       " 'candidate00041',\n",
       " 'candidate00009',\n",
       " 'candidate00029',\n",
       " 'candidate00010',\n",
       " 'candidate00018',\n",
       " 'candidate00020',\n",
       " 'candidate00048',\n",
       " 'candidate00004',\n",
       " 'candidate00047',\n",
       " 'candidate00043',\n",
       " 'candidate00015',\n",
       " 'candidate00050',\n",
       " 'candidate00045',\n",
       " 'candidate00003',\n",
       " 'candidate00023',\n",
       " 'candidate00044',\n",
       " 'candidate00025',\n",
       " 'candidate00021',\n",
       " 'candidate00031',\n",
       " 'candidate00030',\n",
       " 'candidate00003',\n",
       " 'candidate00029',\n",
       " 'candidate00012',\n",
       " 'candidate00043',\n",
       " 'candidate00029',\n",
       " 'candidate00047',\n",
       " 'candidate00022',\n",
       " 'candidate00013',\n",
       " 'candidate00023',\n",
       " 'candidate00040',\n",
       " 'candidate00050',\n",
       " 'candidate00033',\n",
       " 'candidate00037',\n",
       " 'candidate00025',\n",
       " 'candidate00043',\n",
       " 'candidate00047',\n",
       " 'candidate00046',\n",
       " 'candidate00046',\n",
       " 'candidate00043',\n",
       " 'candidate00027',\n",
       " 'candidate00020',\n",
       " 'candidate00015',\n",
       " 'candidate00038',\n",
       " 'candidate00001',\n",
       " 'candidate00048',\n",
       " 'candidate00022',\n",
       " 'candidate00001',\n",
       " 'candidate00009',\n",
       " 'candidate00002',\n",
       " 'candidate00017',\n",
       " 'candidate00045',\n",
       " 'candidate00021',\n",
       " 'candidate00045',\n",
       " 'candidate00038',\n",
       " 'candidate00019',\n",
       " 'candidate00035',\n",
       " 'candidate00040',\n",
       " 'candidate00034',\n",
       " 'candidate00039',\n",
       " 'candidate00003',\n",
       " 'candidate00029',\n",
       " 'candidate00038',\n",
       " 'candidate00048',\n",
       " 'candidate00006',\n",
       " 'candidate00010',\n",
       " 'candidate00026',\n",
       " 'candidate00003',\n",
       " 'candidate00013',\n",
       " 'candidate00014',\n",
       " 'candidate00006',\n",
       " 'candidate00026',\n",
       " 'candidate00042',\n",
       " 'candidate00044',\n",
       " 'candidate00026',\n",
       " 'candidate00014',\n",
       " 'candidate00030',\n",
       " 'candidate00044',\n",
       " 'candidate00027',\n",
       " 'candidate00006',\n",
       " 'candidate00041',\n",
       " 'candidate00044',\n",
       " 'candidate00047',\n",
       " 'candidate00013',\n",
       " 'candidate00050',\n",
       " 'candidate00016',\n",
       " 'candidate00032',\n",
       " 'candidate00003',\n",
       " 'candidate00012',\n",
       " 'candidate00035',\n",
       " 'candidate00033',\n",
       " 'candidate00013',\n",
       " 'candidate00011',\n",
       " 'candidate00023',\n",
       " 'candidate00028',\n",
       " 'candidate00036',\n",
       " 'candidate00049',\n",
       " 'candidate00033',\n",
       " 'candidate00037',\n",
       " 'candidate00032',\n",
       " 'candidate00003',\n",
       " 'candidate00001',\n",
       " 'candidate00022',\n",
       " 'candidate00004',\n",
       " 'candidate00011',\n",
       " 'candidate00035',\n",
       " 'candidate00031',\n",
       " 'candidate00019',\n",
       " 'candidate00040',\n",
       " 'candidate00021',\n",
       " 'candidate00045',\n",
       " 'candidate00024',\n",
       " 'candidate00049',\n",
       " 'candidate00006',\n",
       " 'candidate00020',\n",
       " 'candidate00033',\n",
       " 'candidate00033',\n",
       " 'candidate00012',\n",
       " 'candidate00045',\n",
       " 'candidate00049',\n",
       " 'candidate00035',\n",
       " 'candidate00048',\n",
       " 'candidate00002',\n",
       " 'candidate00041',\n",
       " 'candidate00033',\n",
       " 'candidate00008',\n",
       " 'candidate00037',\n",
       " 'candidate00004',\n",
       " 'candidate00010',\n",
       " 'candidate00029',\n",
       " 'candidate00046',\n",
       " 'candidate00002',\n",
       " 'candidate00041',\n",
       " 'candidate00010',\n",
       " 'candidate00033',\n",
       " 'candidate00042',\n",
       " 'candidate00003',\n",
       " 'candidate00012',\n",
       " 'candidate00007',\n",
       " 'candidate00011',\n",
       " 'candidate00010',\n",
       " 'candidate00030',\n",
       " 'candidate00021',\n",
       " 'candidate00030',\n",
       " 'candidate00027',\n",
       " 'candidate00010',\n",
       " 'candidate00032',\n",
       " 'candidate00019',\n",
       " 'candidate00047',\n",
       " 'candidate00003',\n",
       " 'candidate00003',\n",
       " 'candidate00040',\n",
       " 'candidate00009',\n",
       " 'candidate00004',\n",
       " 'candidate00042',\n",
       " 'candidate00024',\n",
       " 'candidate00019',\n",
       " 'candidate00019',\n",
       " 'candidate00037',\n",
       " 'candidate00023',\n",
       " 'candidate00010',\n",
       " 'candidate00050',\n",
       " 'candidate00026',\n",
       " 'candidate00049',\n",
       " 'candidate00035',\n",
       " 'candidate00017',\n",
       " 'candidate00014',\n",
       " 'candidate00017',\n",
       " 'candidate00022',\n",
       " 'candidate00027',\n",
       " 'candidate00042',\n",
       " 'candidate00033',\n",
       " 'candidate00002',\n",
       " 'candidate00013',\n",
       " 'candidate00018',\n",
       " 'candidate00047',\n",
       " 'candidate00006',\n",
       " 'candidate00006',\n",
       " 'candidate00018',\n",
       " 'candidate00044',\n",
       " 'candidate00039',\n",
       " 'candidate00021',\n",
       " 'candidate00031',\n",
       " 'candidate00026',\n",
       " 'candidate00027',\n",
       " 'candidate00043',\n",
       " 'candidate00016',\n",
       " 'candidate00032',\n",
       " 'candidate00001',\n",
       " 'candidate00009',\n",
       " 'candidate00027',\n",
       " 'candidate00024',\n",
       " 'candidate00018',\n",
       " 'candidate00050',\n",
       " 'candidate00003',\n",
       " 'candidate00047',\n",
       " 'candidate00048',\n",
       " 'candidate00046',\n",
       " 'candidate00021',\n",
       " 'candidate00050',\n",
       " 'candidate00021',\n",
       " 'candidate00043',\n",
       " 'candidate00004',\n",
       " 'candidate00026',\n",
       " 'candidate00035',\n",
       " 'candidate00041',\n",
       " 'candidate00018',\n",
       " 'candidate00032',\n",
       " 'candidate00010',\n",
       " 'candidate00048',\n",
       " 'candidate00005',\n",
       " 'candidate00039',\n",
       " 'candidate00027',\n",
       " 'candidate00021',\n",
       " 'candidate00010',\n",
       " 'candidate00023',\n",
       " 'candidate00007',\n",
       " 'candidate00031',\n",
       " 'candidate00036',\n",
       " 'candidate00047',\n",
       " 'candidate00038',\n",
       " 'candidate00033',\n",
       " 'candidate00010',\n",
       " 'candidate00050',\n",
       " 'candidate00004',\n",
       " 'candidate00020',\n",
       " 'candidate00038',\n",
       " 'candidate00023',\n",
       " 'candidate00020',\n",
       " 'candidate00044',\n",
       " 'candidate00031',\n",
       " 'candidate00024',\n",
       " 'candidate00020',\n",
       " 'candidate00045',\n",
       " 'candidate00025',\n",
       " 'candidate00027',\n",
       " 'candidate00028',\n",
       " 'candidate00030',\n",
       " 'candidate00033',\n",
       " 'candidate00046',\n",
       " 'candidate00031',\n",
       " 'candidate00011',\n",
       " 'candidate00005',\n",
       " 'candidate00024',\n",
       " 'candidate00048',\n",
       " 'candidate00023',\n",
       " 'candidate00031',\n",
       " 'candidate00012',\n",
       " 'candidate00050',\n",
       " 'candidate00011',\n",
       " 'candidate00045',\n",
       " 'candidate00024',\n",
       " 'candidate00035',\n",
       " 'candidate00049',\n",
       " 'candidate00038',\n",
       " 'candidate00031',\n",
       " 'candidate00049',\n",
       " 'candidate00008',\n",
       " 'candidate00018',\n",
       " 'candidate00049',\n",
       " 'candidate00004',\n",
       " 'candidate00020',\n",
       " 'candidate00044',\n",
       " 'candidate00008',\n",
       " 'candidate00019',\n",
       " 'candidate00047',\n",
       " 'candidate00020',\n",
       " 'candidate00015',\n",
       " 'candidate00004',\n",
       " 'candidate00043',\n",
       " 'candidate00022',\n",
       " 'candidate00047',\n",
       " 'candidate00036',\n",
       " 'candidate00043',\n",
       " 'candidate00010',\n",
       " 'candidate00043',\n",
       " 'candidate00028',\n",
       " 'candidate00001',\n",
       " 'candidate00003',\n",
       " 'candidate00046',\n",
       " 'candidate00017',\n",
       " 'candidate00033',\n",
       " 'candidate00013',\n",
       " 'candidate00007',\n",
       " 'candidate00033',\n",
       " 'candidate00025',\n",
       " 'candidate00046',\n",
       " 'candidate00015',\n",
       " 'candidate00049',\n",
       " 'candidate00039',\n",
       " 'candidate00041',\n",
       " 'candidate00040',\n",
       " 'candidate00006',\n",
       " 'candidate00006',\n",
       " 'candidate00041',\n",
       " 'candidate00036',\n",
       " 'candidate00047',\n",
       " 'candidate00046',\n",
       " 'candidate00044',\n",
       " 'candidate00011',\n",
       " 'candidate00009',\n",
       " 'candidate00021',\n",
       " 'candidate00043',\n",
       " 'candidate00037',\n",
       " 'candidate00049',\n",
       " 'candidate00036',\n",
       " 'candidate00017',\n",
       " 'candidate00050',\n",
       " 'candidate00004',\n",
       " 'candidate00010',\n",
       " 'candidate00003',\n",
       " 'candidate00001',\n",
       " 'candidate00003',\n",
       " 'candidate00012',\n",
       " 'candidate00011',\n",
       " 'candidate00026',\n",
       " 'candidate00025',\n",
       " 'candidate00030',\n",
       " 'candidate00004',\n",
       " 'candidate00033',\n",
       " 'candidate00042',\n",
       " 'candidate00003',\n",
       " 'candidate00008',\n",
       " 'candidate00024',\n",
       " 'candidate00008',\n",
       " 'candidate00024',\n",
       " 'candidate00012',\n",
       " 'candidate00030',\n",
       " 'candidate00044',\n",
       " 'candidate00017',\n",
       " 'candidate00006',\n",
       " 'candidate00014',\n",
       " 'candidate00041',\n",
       " 'candidate00024',\n",
       " 'candidate00046',\n",
       " 'candidate00024',\n",
       " 'candidate00035',\n",
       " 'candidate00020',\n",
       " 'candidate00045',\n",
       " 'candidate00036',\n",
       " 'candidate00044',\n",
       " 'candidate00039',\n",
       " 'candidate00019',\n",
       " 'candidate00020',\n",
       " 'candidate00020',\n",
       " 'candidate00040',\n",
       " 'candidate00015',\n",
       " 'candidate00009',\n",
       " 'candidate00017',\n",
       " 'candidate00045',\n",
       " 'candidate00019',\n",
       " 'candidate00050',\n",
       " 'candidate00034',\n",
       " 'candidate00035',\n",
       " 'candidate00024',\n",
       " 'candidate00015',\n",
       " 'candidate00002',\n",
       " 'candidate00022',\n",
       " 'candidate00030',\n",
       " 'candidate00008',\n",
       " 'candidate00007',\n",
       " 'candidate00041',\n",
       " 'candidate00008',\n",
       " 'candidate00001',\n",
       " 'candidate00038',\n",
       " 'candidate00009',\n",
       " 'candidate00001',\n",
       " 'candidate00009',\n",
       " 'candidate00024',\n",
       " 'candidate00011',\n",
       " 'candidate00013',\n",
       " 'candidate00043',\n",
       " 'candidate00011',\n",
       " 'candidate00030',\n",
       " 'candidate00016',\n",
       " 'candidate00003',\n",
       " 'candidate00005',\n",
       " 'candidate00005',\n",
       " 'candidate00046',\n",
       " 'candidate00014',\n",
       " 'candidate00007',\n",
       " 'candidate00003',\n",
       " 'candidate00024',\n",
       " 'candidate00049',\n",
       " 'candidate00016',\n",
       " 'candidate00043',\n",
       " 'candidate00048',\n",
       " 'candidate00032',\n",
       " 'candidate00036',\n",
       " 'candidate00004',\n",
       " 'candidate00034',\n",
       " 'candidate00018',\n",
       " 'candidate00011',\n",
       " 'candidate00015',\n",
       " 'candidate00029',\n",
       " 'candidate00012',\n",
       " 'candidate00018',\n",
       " 'candidate00021',\n",
       " 'candidate00037',\n",
       " 'candidate00010',\n",
       " 'candidate00040',\n",
       " 'candidate00003',\n",
       " 'candidate00020',\n",
       " 'candidate00034',\n",
       " 'candidate00007',\n",
       " 'candidate00004',\n",
       " 'candidate00010',\n",
       " 'candidate00009',\n",
       " 'candidate00002',\n",
       " 'candidate00049',\n",
       " 'candidate00049',\n",
       " 'candidate00035',\n",
       " 'candidate00008',\n",
       " 'candidate00038',\n",
       " 'candidate00010',\n",
       " 'candidate00028',\n",
       " 'candidate00040',\n",
       " 'candidate00028',\n",
       " 'candidate00003',\n",
       " 'candidate00015',\n",
       " 'candidate00031',\n",
       " 'candidate00032',\n",
       " 'candidate00032',\n",
       " 'candidate00008',\n",
       " 'candidate00001',\n",
       " 'candidate00030',\n",
       " 'candidate00038',\n",
       " 'candidate00005',\n",
       " 'candidate00003',\n",
       " 'candidate00014',\n",
       " 'candidate00046',\n",
       " 'candidate00003',\n",
       " 'candidate00006',\n",
       " 'candidate00002',\n",
       " 'candidate00046',\n",
       " 'candidate00011',\n",
       " 'candidate00035',\n",
       " 'candidate00032',\n",
       " 'candidate00019',\n",
       " 'candidate00019',\n",
       " 'candidate00006',\n",
       " 'candidate00017',\n",
       " 'candidate00018',\n",
       " 'candidate00047',\n",
       " 'candidate00007',\n",
       " 'candidate00032',\n",
       " 'candidate00021',\n",
       " 'candidate00033',\n",
       " 'candidate00034',\n",
       " 'candidate00007',\n",
       " 'candidate00033',\n",
       " 'candidate00034',\n",
       " 'candidate00017',\n",
       " 'candidate00037',\n",
       " 'candidate00050',\n",
       " 'candidate00044',\n",
       " 'candidate00006',\n",
       " 'candidate00045',\n",
       " 'candidate00043',\n",
       " 'candidate00028',\n",
       " 'candidate00019',\n",
       " 'candidate00034',\n",
       " 'candidate00043',\n",
       " 'candidate00004',\n",
       " 'candidate00028',\n",
       " 'candidate00034',\n",
       " 'candidate00022',\n",
       " 'candidate00045',\n",
       " 'candidate00037',\n",
       " 'candidate00003',\n",
       " 'candidate00042',\n",
       " 'candidate00023',\n",
       " 'candidate00031',\n",
       " 'candidate00001',\n",
       " 'candidate00042',\n",
       " 'candidate00012',\n",
       " 'candidate00038',\n",
       " 'candidate00017',\n",
       " 'candidate00023',\n",
       " 'candidate00026',\n",
       " 'candidate00021',\n",
       " 'candidate00005',\n",
       " 'candidate00045',\n",
       " 'candidate00028',\n",
       " 'candidate00010',\n",
       " 'candidate00037',\n",
       " 'candidate00008',\n",
       " 'candidate00014',\n",
       " 'candidate00011',\n",
       " 'candidate00043',\n",
       " 'candidate00027',\n",
       " 'candidate00049',\n",
       " 'candidate00020',\n",
       " 'candidate00012',\n",
       " 'candidate00015',\n",
       " 'candidate00008',\n",
       " 'candidate00010',\n",
       " 'candidate00011',\n",
       " 'candidate00037',\n",
       " 'candidate00011',\n",
       " 'candidate00021',\n",
       " 'candidate00009',\n",
       " 'candidate00047',\n",
       " 'candidate00004',\n",
       " 'candidate00021',\n",
       " 'candidate00037',\n",
       " 'candidate00031',\n",
       " 'candidate00004',\n",
       " 'candidate00015',\n",
       " 'candidate00048',\n",
       " 'candidate00004',\n",
       " 'candidate00023',\n",
       " 'candidate00047',\n",
       " 'candidate00012',\n",
       " 'candidate00015',\n",
       " 'candidate00016',\n",
       " 'candidate00027',\n",
       " 'candidate00013',\n",
       " 'candidate00033',\n",
       " 'candidate00044',\n",
       " 'candidate00026',\n",
       " 'candidate00022',\n",
       " 'candidate00048',\n",
       " 'candidate00040',\n",
       " 'candidate00002',\n",
       " 'candidate00043',\n",
       " 'candidate00048',\n",
       " 'candidate00016',\n",
       " 'candidate00027',\n",
       " 'candidate00036',\n",
       " 'candidate00034',\n",
       " 'candidate00042',\n",
       " 'candidate00001',\n",
       " 'candidate00020',\n",
       " 'candidate00008',\n",
       " 'candidate00017',\n",
       " 'candidate00040',\n",
       " 'candidate00034',\n",
       " 'candidate00011',\n",
       " 'candidate00026',\n",
       " 'candidate00050',\n",
       " 'candidate00037',\n",
       " 'candidate00046',\n",
       " 'candidate00027',\n",
       " 'candidate00027',\n",
       " 'candidate00017',\n",
       " 'candidate00037',\n",
       " 'candidate00039',\n",
       " 'candidate00018',\n",
       " 'candidate00003',\n",
       " 'candidate00005',\n",
       " 'candidate00006',\n",
       " 'candidate00010',\n",
       " 'candidate00042',\n",
       " 'candidate00049',\n",
       " 'candidate00021',\n",
       " 'candidate00031',\n",
       " 'candidate00022',\n",
       " 'candidate00047',\n",
       " 'candidate00010',\n",
       " 'candidate00023',\n",
       " 'candidate00042',\n",
       " 'candidate00044',\n",
       " 'candidate00018',\n",
       " 'candidate00002',\n",
       " 'candidate00015',\n",
       " 'candidate00007',\n",
       " 'candidate00029',\n",
       " 'candidate00033',\n",
       " 'candidate00022',\n",
       " 'candidate00019',\n",
       " 'candidate00036',\n",
       " 'candidate00047',\n",
       " 'candidate00020',\n",
       " 'candidate00014',\n",
       " 'candidate00045',\n",
       " 'candidate00050',\n",
       " 'candidate00029',\n",
       " 'candidate00039',\n",
       " 'candidate00012',\n",
       " 'candidate00044',\n",
       " 'candidate00004',\n",
       " 'candidate00039',\n",
       " 'candidate00050',\n",
       " 'candidate00010',\n",
       " 'candidate00017',\n",
       " 'candidate00044',\n",
       " 'candidate00050',\n",
       " 'candidate00021',\n",
       " 'candidate00045',\n",
       " 'candidate00029',\n",
       " 'candidate00007',\n",
       " 'candidate00034',\n",
       " 'candidate00008',\n",
       " 'candidate00030',\n",
       " 'candidate00043',\n",
       " 'candidate00016',\n",
       " 'candidate00039']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_authors = []\n",
    "for index, row in test_df.iterrows():\n",
    "    if row['Text'] in set(full_data['Text']):\n",
    "        author = full_data.loc[full_data['Text'] == row['Text'], 'Author'].values[0]\n",
    "        list_of_authors.append(author)\n",
    "list_of_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Top K Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_words(text, w2v_model, clf_model, k):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stopwords and words not in the Word2Vec model\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english') and token in w2v_model.key_to_index]\n",
    "    # Get the Word2Vec embeddings for the tokens\n",
    "    X = w2v_model[tokens]\n",
    "    # Get the feature importances for the tokens using the classifier model\n",
    "    feature_importances = clf_model.coef_.mean(axis=0)\n",
    "    # Sort the tokens by their feature importances\n",
    "    sorted_words = sorted(zip(tokens, feature_importances), key=lambda x: x[1], reverse=True)\n",
    "    # Return the top k words and their feature importance values\n",
    "    return sorted_words[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v...\n"
     ]
    }
   ],
   "source": [
    "X = data['Text']\n",
    "y = data['Author']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "print('w2v...')\n",
    "sentences = X_train.apply(get_sentence_embedding).tolist()\n",
    "# sentences = X_train.apply(lambda x: x.split()).tolist()\n",
    "# w2v_model = Word2Vec(sentences, vector_size=200, window=5, min_count=1, workers=4)\n",
    "\n",
    "\n",
    "w2v_model = model#Word2Vec.load('wiki-news-300d-1M-subword.vec\\wiki-news-300d-1M-subword.vec',)\n",
    "\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(sentences, y_train)\n",
    "clf_model = clf#pickle.load(open('classifier_model.pkl', 'rb'))\n",
    "\n",
    "test_sentences = X_test.apply(get_sentence_embedding).tolist()\n",
    "test =test_sentences[0].reshape(1,-1)\n",
    "yprob = clf_model.predict_proba(test)\n",
    "# accuracy_score(y_test, y_pred)\n",
    "ypred = clf_model.predict(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test =X_test.reset_index(drop=True)\n",
    "y_test =y_test.reset_index(drop=True)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 words and their feature importance values for a text sample\n",
    "print(\"top k words\")\n",
    "text = X_test.iloc[22]#\"This is a new sample text\"\n",
    "label = y_test.iloc[22]\n",
    "top_k_words = get_top_k_words(text, w2v_model , clf_model, 55)\n",
    "\n",
    "# Print the top k words and their feature importance values\n",
    "for word, importance in top_k_words:\n",
    "    print(f\"{word}: {importance}\")\n",
    "label, text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Top Synonyms (of top k words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def find_top_n_synonyms(top_k_words, w2v_model, n):\n",
    "    # Get the indices of the top k words in the text\n",
    "    top_k_indices = [w2v_model.key_to_index[word] for word, _ in top_k_words]\n",
    "    \n",
    "    # Find the top n synonyms for each of the top k words\n",
    "    top_n_synonyms = {}\n",
    "    for word_index in top_k_indices:\n",
    "        word = w2v_model.index_to_key[word_index]\n",
    "        synonyms = []\n",
    "        for synset in wn.synsets(word):\n",
    "            for lemma in synset.lemmas():\n",
    "                syn_word = lemma.name().replace('_', ' ').lower()\n",
    "                if syn_word != word and syn_word in w2v_model:\n",
    "                    similarity = w2v_model.similarity(word, syn_word)\n",
    "                    synonyms.append((syn_word, similarity))\n",
    "        synonyms = sorted(synonyms, key=lambda x: x[1], reverse=True)[:n]\n",
    "        if synonyms:\n",
    "            top_n_synonyms[word] = synonyms\n",
    "    \n",
    "    return top_n_synonyms\n",
    "\n",
    "\n",
    "\n",
    "# Note that this function uses the NLTK library's WordNet module to find synonyms and their semantic similarity scores.\n",
    "# The function takes in a parameter n to control the number of synonyms to find for each word. The output of the function is a\n",
    "# dictionary that maps each of the top k words to a list of its top n synonyms, along with their semantic similarity scores.\n",
    "#In this version of the function, the inner loop only adds lemmas to the synonyms list if they are not equal to the original word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn = find_top_n_synonyms(top_k_words, w2v_model, 25)\n",
    "syn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries and functions\n",
    "from typing import List, Dict, Set\n",
    "import numpy as np\n",
    "\n",
    "# import necessary libraries and functions\n",
    "from typing import List, Dict, Set\n",
    "import numpy as np\n",
    "\n",
    "def substitute_words(text_sample: str, topk: List[str], topsyn: Dict[str, Set[str]], classifier_model, w2v_model) -> str:\n",
    "    # Convert the text sample to word embeddings\n",
    "    embeddings = embedding_function(text_sample, w2v_model)\n",
    "    \n",
    "    # Predict the probability estimates of each class with the classifier model\n",
    "    original_proba = classifier_model.predict_proba(embeddings.reshape(1,-1))\n",
    "    \n",
    "    # Get the index of the class with the highest probability\n",
    "    original_label = np.argmax(original_proba)\n",
    "    \n",
    "    drops = {}\n",
    "    for word in topk:\n",
    "        if word not in w2v_model:\n",
    "            continue\n",
    "        else:\n",
    "            print(\"HERE\")\n",
    "            for syn in topsyn[word[0]]:\n",
    "                # Replace the word with its synonym\n",
    "                new_text_sample = text_sample.replace(word, syn)\n",
    "                \n",
    "                # Convert the new text sample to word embeddings\n",
    "                new_embeddings = []\n",
    "                for word in new_text_sample.split():\n",
    "                    if word in w2v_model:\n",
    "                        new_embeddings.append(w2v_model[word])\n",
    "                    else:\n",
    "                        new_embeddings.append(w2v_model[w2v_model.index2word.index(word)])\n",
    "                new_embeddings = np.array(new_embeddings)            \n",
    "                # Predict the probability estimates of each class with the classifier model\n",
    "                new_proba = classifier_model.predict_proba(new_embeddings)\n",
    "                \n",
    "                # Calculate the probability difference between the new label and the original label\n",
    "                prob_diff = original_proba[0, original_label] - new_proba[0, original_label]\n",
    "                print(prob_diff)\n",
    "                # If the probability difference is negative, add the word to the \"drops\" dictionary\n",
    "                if prob_diff < 0:\n",
    "                    drops[word] = prob_diff\n",
    "                \n",
    "        # If there are words in the \"drops\" dictionary, choose the one that caused the biggest probability decrease and substitute it into the text sample\n",
    "        if drops:\n",
    "            max_drop_word = max(drops, key=drops.get)\n",
    "            text_sample = text_sample.replace(max_drop_word, list(topsyn[max_drop_word])[0])\n",
    "            drops = {}  # Reset the \"drops\" dictionary for the next word in topk\n",
    "            \n",
    "    # Convert the edited text sample to word embeddings\n",
    "    edited_embeddings = embedding_function(text_sample, w2v_model)\n",
    "    \n",
    "    # Predict the probability estimates of each class for the edited text with the classifier model\n",
    "    edited_proba = classifier_model.predict_proba(edited_embeddings.reshape(1,-1))\n",
    "    \n",
    "    # Get the index of the class with the highest probability for the edited text\n",
    "    edited_label = np.argmax(edited_proba)\n",
    "    \n",
    "    # Compare the edited label to the original label\n",
    "    if edited_label == original_label:\n",
    "        return \"Unable to misclassify\"\n",
    "    else:\n",
    "        return f\"New label: {edited_label}, New text: {text_sample}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unable to misclassify'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = X_test.iloc[0]\n",
    "# print(text)\n",
    "substitute_words(text, top_k_words, syn, clf_model, w2v_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
