{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, Counter, OrderedDict\n",
    "import csv\n",
    "import empath\n",
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "# from pandas.io.json import json_normalize\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "import string\n",
    "\n",
    "\n",
    "# import gensim\n",
    "# from gensim.models.wrappers import LdaMallet\n",
    "# import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lazypredict\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.manifold import Isomap, LocallyLinearEmbedding, TSNE\n",
    "from sklearn.metrics import accuracy_score, davies_bouldin_score, calinski_harabasz_score, silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, Normalizer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(r\"data\\c50.csv\")\n",
    "test_data = pd.read_csv(r\"data\\c50_Test.csv\")\n",
    "test_data = test_data.drop(test_data.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>The Internet may be overflowing with new techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>The U.S. Postal Service announced Wednesday a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>Elementary school students with access to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>An influential Internet organisation has backe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>An influential Internet organisation has backe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>candidate00050</td>\n",
       "      <td>China's central bank chief has said that infla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>candidate00050</td>\n",
       "      <td>China ushered in 1997, a year it has hailed as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>candidate00050</td>\n",
       "      <td>China issued tough new rules on the handling o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>candidate00050</td>\n",
       "      <td>China will avoid bold moves in tackling its ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>candidate00050</td>\n",
       "      <td>Communist Party chief Jiang Zemin has put his ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Author                                               Text\n",
       "0     candidate00001  The Internet may be overflowing with new techn...\n",
       "1     candidate00001  The U.S. Postal Service announced Wednesday a ...\n",
       "2     candidate00001  Elementary school students with access to the ...\n",
       "3     candidate00001  An influential Internet organisation has backe...\n",
       "4     candidate00001  An influential Internet organisation has backe...\n",
       "...              ...                                                ...\n",
       "2495  candidate00050  China's central bank chief has said that infla...\n",
       "2496  candidate00050  China ushered in 1997, a year it has hailed as...\n",
       "2497  candidate00050  China issued tough new rules on the handling o...\n",
       "2498  candidate00050  China will avoid bold moves in tackling its ai...\n",
       "2499  candidate00050  Communist Party chief Jiang Zemin has put his ...\n",
       "\n",
       "[2500 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>candidate00046</td>\n",
       "      <td>China and Britain agreed on Wednesday to relea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>The Federal Reserve may not be taking adequate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>candidate00009</td>\n",
       "      <td>Britain's motor industry reported 1996 car reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>candidate00019</td>\n",
       "      <td>When the former Czechoslovak diplomat Josef Ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>candidate00012</td>\n",
       "      <td>China is building a network of major toll high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>candidate00018</td>\n",
       "      <td>Britain's big banks look set to raise profits ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>candidate00047</td>\n",
       "      <td>After two years of hype and euphoria about the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>candidate00002</td>\n",
       "      <td>Czech annual average consumer inflation eased ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>candidate00037</td>\n",
       "      <td>Kellogg Co, whose profits for 1996 are under p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>candidate00018</td>\n",
       "      <td>London-based international bank HSBC Holdings ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Author                                               Text\n",
       "0     candidate00046  China and Britain agreed on Wednesday to relea...\n",
       "1     candidate00001  The Federal Reserve may not be taking adequate...\n",
       "2     candidate00009  Britain's motor industry reported 1996 car reg...\n",
       "3     candidate00019  When the former Czechoslovak diplomat Josef Ko...\n",
       "4     candidate00012  China is building a network of major toll high...\n",
       "...              ...                                                ...\n",
       "2495  candidate00018  Britain's big banks look set to raise profits ...\n",
       "2496  candidate00047  After two years of hype and euphoria about the...\n",
       "2497  candidate00002  Czech annual average consumer inflation eased ...\n",
       "2498  candidate00037  Kellogg Co, whose profits for 1996 are under p...\n",
       "2499  candidate00018  London-based international bank HSBC Holdings ...\n",
       "\n",
       "[2500 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = list(test_data.columns)\n",
    "cols = [cols[-1]] + cols[:-1]\n",
    "test_data = test_data.reindex(columns=cols)\n",
    "test_data = test_data.rename(columns={'true-author': 'Author'})\n",
    "test_data = test_data.drop(columns='ID', axis=1)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>The Internet may be overflowing with new techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>The U.S. Postal Service announced Wednesday a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>Elementary school students with access to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>An influential Internet organisation has backe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>An influential Internet organisation has backe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>candidate00018</td>\n",
       "      <td>Britain's big banks look set to raise profits ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>candidate00047</td>\n",
       "      <td>After two years of hype and euphoria about the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>candidate00002</td>\n",
       "      <td>Czech annual average consumer inflation eased ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>candidate00037</td>\n",
       "      <td>Kellogg Co, whose profits for 1996 are under p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>candidate00018</td>\n",
       "      <td>London-based international bank HSBC Holdings ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Author                                               Text\n",
       "0     candidate00001  The Internet may be overflowing with new techn...\n",
       "1     candidate00001  The U.S. Postal Service announced Wednesday a ...\n",
       "2     candidate00001  Elementary school students with access to the ...\n",
       "3     candidate00001  An influential Internet organisation has backe...\n",
       "4     candidate00001  An influential Internet organisation has backe...\n",
       "...              ...                                                ...\n",
       "2495  candidate00018  Britain's big banks look set to raise profits ...\n",
       "2496  candidate00047  After two years of hype and euphoria about the...\n",
       "2497  candidate00002  Czech annual average consumer inflation eased ...\n",
       "2498  candidate00037  Kellogg Co, whose profits for 1996 are under p...\n",
       "2499  candidate00018  London-based international bank HSBC Holdings ...\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = pd.concat([train_data, test_data])\n",
    "full_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove leading and trailing whitespace\n",
    "full_data['Text'] = full_data['Text'].str.strip()\n",
    "\n",
    "# replace multiple spaces with a single space\n",
    "full_data['Text'] = full_data['Text'].str.replace(r'\\s+', ' ')\n",
    "\n",
    "remove_emails = lambda x: re.sub(r'\\S+@\\S+', '', x)\n",
    "remove_urls = lambda x: re.sub(r'http\\S+', '', x)\n",
    "\n",
    "full_data['Text'] = full_data['Text'].apply(remove_emails).apply(remove_urls)\n",
    "full_data['Text'] = full_data['Text'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x.lower()))\n",
    "full_data['Text'] = full_data['Text'].str.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>the internet may be overflowing with new techn...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>the us postal service announced wednesday a pl...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>elementary school students with access to the ...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>an influential internet organisation has backe...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>candidate00001</td>\n",
       "      <td>an influential internet organisation has backe...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>candidate00018</td>\n",
       "      <td>britains big banks look set to raise profits b...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>candidate00047</td>\n",
       "      <td>after two years of hype and euphoria about the...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>candidate00002</td>\n",
       "      <td>czech annual average consumer inflation eased ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>candidate00037</td>\n",
       "      <td>kellogg co whose profits for 1996 are under pr...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>candidate00018</td>\n",
       "      <td>londonbased international bank hsbc holdings p...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Author                                               Text   \n",
       "0     candidate00001  the internet may be overflowing with new techn...  \\\n",
       "1     candidate00001  the us postal service announced wednesday a pl...   \n",
       "2     candidate00001  elementary school students with access to the ...   \n",
       "3     candidate00001  an influential internet organisation has backe...   \n",
       "4     candidate00001  an influential internet organisation has backe...   \n",
       "...              ...                                                ...   \n",
       "4995  candidate00018  britains big banks look set to raise profits b...   \n",
       "4996  candidate00047  after two years of hype and euphoria about the...   \n",
       "4997  candidate00002  czech annual average consumer inflation eased ...   \n",
       "4998  candidate00037  kellogg co whose profits for 1996 are under pr...   \n",
       "4999  candidate00018  londonbased international bank hsbc holdings p...   \n",
       "\n",
       "      word_count  \n",
       "0             18  \n",
       "1             25  \n",
       "2             30  \n",
       "3             23  \n",
       "4             23  \n",
       "...          ...  \n",
       "4995          37  \n",
       "4996          31  \n",
       "4997          19  \n",
       "4998          37  \n",
       "4999          23  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_func = lambda x: len(x.split())\n",
    "full_data['word_count'] = full_data['Text'].apply(word_count_func)\n",
    "full_data = full_data.reset_index(drop=True)\n",
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = pd.Categorical(full_data['Author']).categories\n",
    "full_data['encoded_Author'] = pd.Categorical(full_data['Author'], categories=categories).codes\n",
    "\n",
    "full_texts = full_data['Text'].to_list()\n",
    "full_labels = full_data['encoded_Author'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 1000, 1000, 3000, 1000, 1000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = full_data['Text']#.drop(['Author', 'encoded_author'], axis=1)\n",
    "y = full_data['encoded_Author']\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split train into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "len(X_train), len(X_test), len(X_val), len(y_train), len(y_test), len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = list(train_data['Text'])\n",
    "train_auth = list(train_data['Author'])\n",
    "\n",
    "test_text = list(test_data['Text'])\n",
    "test_auth = list(test_data['Author'])\n",
    "\n",
    "full_text = list(full_data['Text'])\n",
    "full_auth = list(full_data['Author'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "# pca = PCA(n_components=200, random_state=42)\n",
    "svd = TruncatedSVD(n_components=2, n_iter=75, random_state=42)\n",
    "tfidf_full = tfidf.fit_transform(full_text)\n",
    "# tfidf_train_dense = tfidf_train.todense()\n",
    "# tfidf_train_pca = pca.fit_transform(tfidf_train_dense)\n",
    "tfidf_full_svd = svd.fit_transform(tfidf_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 1000, 1000, 3000, 1000, 1000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FOR FULL_DATA\n",
    "X = tfidf_full_svd#full_data['Text']#.drop(['Author', 'encoded_author'], axis=1)\n",
    "y = full_data['Author']#['cluster']\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split train into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "len(X_train), len(X_test), len(X_val), len(y_train), len(y_test), len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:13<00:00,  2.21it/s]\n"
     ]
    }
   ],
   "source": [
    "lc = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = lc.fit(pd.DataFrame(X_train), pd.DataFrame(X_test), list(y_train), list(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>None</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesClassifier</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>None</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>None</td>\n",
       "      <td>0.13</td>\n",
       "      <td>3.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>None</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingClassifier</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>None</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>None</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuadraticDiscriminantAnalysis</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>None</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>None</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>None</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>None</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearestCentroid</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>None</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSVC</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>None</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>None</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>None</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CalibratedClassifierCV</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>None</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearDiscriminantAnalysis</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>None</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>None</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifier</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>None</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifierCV</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>None</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveClassifier</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>None</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  Balanced Accuracy ROC AUC  F1 Score   \n",
       "Model                                                                          \n",
       "RandomForestClassifier             0.13               0.13    None      0.13  \\\n",
       "ExtraTreesClassifier               0.13               0.13    None      0.13   \n",
       "LGBMClassifier                     0.13               0.13    None      0.13   \n",
       "DecisionTreeClassifier             0.12               0.12    None      0.12   \n",
       "BaggingClassifier                  0.12               0.12    None      0.12   \n",
       "ExtraTreeClassifier                0.12               0.12    None      0.12   \n",
       "QuadraticDiscriminantAnalysis      0.09               0.09    None      0.06   \n",
       "SVC                                0.09               0.09    None      0.06   \n",
       "KNeighborsClassifier               0.09               0.08    None      0.08   \n",
       "GaussianNB                         0.08               0.08    None      0.05   \n",
       "NearestCentroid                    0.08               0.07    None      0.05   \n",
       "NuSVC                              0.07               0.07    None      0.06   \n",
       "LogisticRegression                 0.07               0.07    None      0.04   \n",
       "LinearSVC                          0.07               0.07    None      0.03   \n",
       "CalibratedClassifierCV             0.07               0.07    None      0.04   \n",
       "LinearDiscriminantAnalysis         0.06               0.07    None      0.04   \n",
       "BernoulliNB                        0.06               0.05    None      0.01   \n",
       "AdaBoostClassifier                 0.06               0.05    None      0.02   \n",
       "RidgeClassifier                    0.05               0.05    None      0.02   \n",
       "RidgeClassifierCV                  0.05               0.05    None      0.02   \n",
       "SGDClassifier                      0.04               0.04    None      0.01   \n",
       "PassiveAggressiveClassifier        0.04               0.03    None      0.01   \n",
       "Perceptron                         0.03               0.03    None      0.01   \n",
       "DummyClassifier                    0.02               0.02    None      0.00   \n",
       "\n",
       "                               Time Taken  \n",
       "Model                                      \n",
       "RandomForestClassifier               0.97  \n",
       "ExtraTreesClassifier                 0.62  \n",
       "LGBMClassifier                       3.22  \n",
       "DecisionTreeClassifier               0.04  \n",
       "BaggingClassifier                    0.17  \n",
       "ExtraTreeClassifier                  0.02  \n",
       "QuadraticDiscriminantAnalysis        0.02  \n",
       "SVC                                  0.66  \n",
       "KNeighborsClassifier                 0.04  \n",
       "GaussianNB                           0.02  \n",
       "NearestCentroid                      0.03  \n",
       "NuSVC                                1.25  \n",
       "LogisticRegression                   0.13  \n",
       "LinearSVC                            0.79  \n",
       "CalibratedClassifierCV               3.62  \n",
       "LinearDiscriminantAnalysis           0.02  \n",
       "BernoulliNB                          0.02  \n",
       "AdaBoostClassifier                   0.71  \n",
       "RidgeClassifier                      0.03  \n",
       "RidgeClassifierCV                    0.04  \n",
       "SGDClassifier                        0.16  \n",
       "PassiveAggressiveClassifier          0.07  \n",
       "Perceptron                           0.07  \n",
       "DummyClassifier                      0.02  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0728"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = TfidfVectorizer()\n",
    "svd = TruncatedSVD(n_components=2, n_iter=7, random_state=42)\n",
    "lsa = make_pipeline(vec, svd)\n",
    "\n",
    "clf = SVC(C=150, gamma=2e-2, probability=True)\n",
    "pipe = make_pipeline(vec, svd, clf)\n",
    "pipe.fit(train_text, train_auth)\n",
    "pipe.score(test_text, test_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction(doc):\n",
    "    y_pred = pipe.predict_proba([doc])[0]\n",
    "    for target, prob in zip(test_auth, y_pred):\n",
    "        print(\"{:.3f} {}\".format(prob, target))\n",
    "\n",
    "doc = exp_text#test_text[4]\n",
    "print_prediction(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_data['Author'].unique()\n",
    "explainer = LimeTextExplainer(class_names=classes)\n",
    "exp_text = test_text[4]\n",
    "lime_exp = explainer.explain_instance(exp_text, pipe.predict_proba, top_labels=1)#pipe.predict_proba)#, num_features=(len(exp_text)//5))\n",
    "# pipe.predict_proba(exp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_exp.show_in_notebook()\n",
    "# test_auth[4]\n",
    "pred = lime_exp.top_labels\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('China', 0.10385065678995657),\n",
       " ('said', 0.035556795701384945),\n",
       " ('analysts', 0.021809420333518296),\n",
       " ('expansion', -0.01924900765830568),\n",
       " ('Monday', 0.01912818641042654),\n",
       " ('float', -0.01569811679007957),\n",
       " ('fund', -0.006403119697049611),\n",
       " ('network', -0.004723409506969597),\n",
       " ('plans', 0.004157887679415947),\n",
       " ('stock', 0.003607507104793779)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_importances = lime_exp.as_list(pred[0])\n",
    "word_importances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 999994\n",
      "Vector size: 300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "999994"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# load the pre-trained FastText model\n",
    "model_path = r\"wiki-news-300d-1M-subword.vec\\wiki-news-300d-1M-subword.vec\"\n",
    "vec_model = KeyedVectors.load_word2vec_format(model_path, binary=False)\n",
    "\n",
    "# check the size of the vocabulary and the vector size\n",
    "print(f\"Vocabulary size: {len(vec_model.index_to_key)}\")\n",
    "print(f\"Vector size: {vec_model.vector_size}\")\n",
    "len(vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(sentence):\n",
    "    # Tokenize the sentence into individual words\n",
    "    tokens = sentence.split()\n",
    "    # Get the embedding for each word in the sentence\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in vec_model:\n",
    "            embeddings.append(vec_model[token])\n",
    "    # Calculate the average embedding for the sentence\n",
    "    if len(embeddings) > 0:\n",
    "        sentence_embedding = np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        sentence_embedding = np.zeros((300,))\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 300)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the embeddings for each sentence\n",
    "w2v_full = full_data['Text'].apply(get_sentence_embedding).tolist()\n",
    "\n",
    "# Convert the embeddings list into a numpy array\n",
    "w2v_full = np.array(w2v_full)\n",
    "\n",
    "# Print the shape of the embeddings array\n",
    "print(w2v_full.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_w2v = TruncatedSVD(n_components=2, n_iter=75, random_state=42)\n",
    "w2v_full = svd_w2v.fit_transform(w2v_full)\n",
    "clf = LinearSVC()\n",
    "w2v_pipe = make_pipeline(clf)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the higher the better\n",
    "def sil_score(num_clusters, df):\n",
    "    algort = KMeans(num_clusters).fit(df)\n",
    "    print(silhouette_score(df,algort.labels_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribution Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attribution_score(word_attributions):\n",
    "    total = 0\n",
    "    for _, value in word_attributions:\n",
    "        total += value\n",
    "    return total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Top Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "def get_topk(text, word_attributions):\n",
    "    # recombined_words = get_full_words(text, word_attributions)\n",
    "    top_words = []\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    doc = nlp(text) #the most recent sentence form; keep to revert back to after subbing a word\n",
    "    doc_ents = [str(ent) for ent in doc.ents] #get the entities to skip over\n",
    "\n",
    "\n",
    "\n",
    "    sorted_words = sorted(word_attributions, key=lambda x: x[1], reverse=True)\n",
    "    positive_words = [t[0] for t in sorted_words if t[1] > 0]\n",
    "\n",
    "    for word in positive_words: #loop through top k words\n",
    "        if word in doc_ents or word in stop_words or any(punc in word for punc in string.punctuation): #if key is an entity, skip it\n",
    "            # print(\"NER\")\n",
    "            continue\n",
    "        else:\n",
    "            top_words.append(word)\n",
    "    \n",
    "    return top_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Synonyms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to get the top k synonyms for a word\n",
    "def get_synonyms_wn(word, k=5):\n",
    "    # Get the synsets for the word\n",
    "    synsets = wordnet.synsets(word)\n",
    "    # Get the lemmas for each synset and add them to a set\n",
    "    synonyms = set()\n",
    "    for synset in synsets:\n",
    "        for lemma in synset.lemmas():\n",
    "            # Lemmatize the lemma and add it to the set of synonyms\n",
    "            lemma = lemmatizer.lemmatize(lemma.name())\n",
    "            if lemma != word:\n",
    "                synonyms.add(lemma)\n",
    "    # Convert the set to a list and return the top k synonyms\n",
    "    synonyms = [syn.replace(\"_\", \" \") for syn in synonyms]\n",
    "    return list(synonyms)[:k]\n",
    "\n",
    "\n",
    "def get_syn_dict_wn(words):\n",
    "    # Dictionary to hold the synonyms\n",
    "    synonyms_dict = {}\n",
    "\n",
    "    # Get the synonyms for each word and add them to the dictionary\n",
    "    for word in words:\n",
    "        synonyms = get_synonyms_wn(word)\n",
    "        synonyms_dict[word] = synonyms\n",
    "\n",
    "    # Print the dictionary\n",
    "    # print(synonyms_dict)\n",
    "    return synonyms_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText Model with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# load the FastText model\n",
    "# model = KeyedVectors.load_word2vec_format('path/to/fasttext.bin', binary=True)\n",
    "\n",
    "def get_synonyms_kv(word, k=5):\n",
    "    try:\n",
    "        # get the k most similar words to the given word from the FastText model\n",
    "        synonyms = vec_model.most_similar(word, topn=k)\n",
    "        return [synonym[0] for synonym in synonyms]\n",
    "    except KeyError:\n",
    "        # handle the case where the word is not in the FastText model\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_syn_dict_kv(words):\n",
    "    # Dictionary to hold the synonyms\n",
    "    synonyms_dict = {}\n",
    "\n",
    "    # Get the synonyms for each word and add them to the dictionary\n",
    "    for word in words:\n",
    "        synonyms = get_synonyms_kv(word)\n",
    "        synonyms_dict[word] = synonyms\n",
    "\n",
    "    # Print the dictionary\n",
    "    # print(synonyms_dict)\n",
    "    return synonyms_dict\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sandbox\n",
    "import spacy\n",
    "import re\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# NER = ['ORG', 'PERSON', 'GPE', 'DATE', 'TIME', 'PRODUCT']\n",
    "# sent = \"Today is a Tuesday , tomorrow will be Wednesday. We will be going to Mercury Finance Co. on Thursday!\"\n",
    "# topkwords = {\"Tuesday\": [\"Thursday\", \"BLANK\"], \"be\": [\"is\", \"BLANK\"], \"Mercury Finance Co.\": [\"BLANK\"]} #{topk: [synonyms]}\n",
    "\n",
    "# orig_attrib_score = get_attribution_score(word_attributions)\n",
    "# orig_label = true_auth\n",
    "# new_label = \"\"\n",
    "# curr_sent = sent #the most recent sentence form; keep to revert back to after subbing a word\n",
    "# test_sent = \"\" #for testing out replacement words\n",
    "# new_sent = \"\" #what is returned with the best subs\n",
    "\n",
    "\n",
    "def sub_words(original_text,synonym_dict, original_label, original_attribution_score):\n",
    "    # doc = nlp(original_text) #the most recent sentence form; keep to revert back to after subbing a word\n",
    "    # doc_ents = [str(ent) for ent in doc.ents] #get the entities to skip over\n",
    "\n",
    "    curr_text = original_text\n",
    "    curr_text = re.sub(r'\\s+', ' ', curr_text)\n",
    "    \n",
    "    test_text = \"\" #for testing out replacement words\n",
    "    new_label = \"\"\n",
    "\n",
    "    for key in synonym_dict: #loop through top k words\n",
    "    #     if key in doc_ents: #if key is an entity, skip it\n",
    "    #         # print(\"NER\")\n",
    "    #         continue\n",
    "        if synonym_dict[key]:#make sure the list of synonyms for a key isn't empty\n",
    "            for value in synonym_dict[key]: #loop through list of values for each top k word\n",
    "                test_text = curr_text.replace(key, value) #replace keyword\n",
    "\n",
    "                lime_exp = explainer.explain_instance(test_text, pipe.predict_proba, top_labels=1,num_features=word_count_func(test_text))\n",
    "                pred = lime_exp.top_labels[0]\n",
    "\n",
    "                word_attributions = lime_exp.as_list(pred)\n",
    "\n",
    "                # word_attributions = cls_explainer(test_text) #reclassify with replacement\n",
    "                # del word_attributions[0]\n",
    "                \n",
    "                new_attrib_score = get_attribution_score(word_attributions) #get new attribution score\n",
    "                new_label = lime_exp.top_labels[0]\n",
    "                # new_label = cls_explainer.predicted_class_name\n",
    "\n",
    "                if new_label != original_label: #SUCCESS\n",
    "                    print(\"SUCCESS!\")\n",
    "                    curr_text = test_text\n",
    "                    return curr_text\n",
    "                \n",
    "                elif new_attrib_score < original_attribution_score:\n",
    "                    curr_text = test_text #updating the current sentence with the new word that caused the attribution score to drop\n",
    "                else: #reset sentence back to previous to try other values if the score doesnt drop or the label doesnt change\n",
    "                    test_text = curr_text \n",
    "        else:#go to the next key\n",
    "            continue\n",
    "    print(\"Unable to change label.\")\n",
    "    return curr_text#, new_label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through multiple samples\n",
    "results = pd.DataFrame(columns=['Original Label', 'Original Attribution Score', 'Original Text', 'New Label', 'New Attribution Score', 'New text', 'Misclassified'])\n",
    "\n",
    "def generate_adv(text):\n",
    "    wc = word_count_func(text)\n",
    "    lime_exp = explainer.explain_instance(text, pipe.predict_proba, top_labels=1,num_features=wc)\n",
    "    original_label = lime_exp.top_labels[0]\n",
    "    word_attributions = lime_exp.as_list(original_label)\n",
    "    # word_attributions = cls_explainer(text)\n",
    "    # del word_attributions[0]\n",
    "    original_attribution_score = get_attribution_score(word_attributions)\n",
    "\n",
    "\n",
    "    top_words = get_topk(text, word_attributions)\n",
    "    \n",
    "    syn_dict = get_syn_dict_wn(top_words)\n",
    "    # syn_dict = get_syn_dict_kv(top_words)\n",
    "\n",
    "    adv = sub_words(text, syn_dict, lime_exp.top_labels, original_attribution_score)\n",
    "    wc = word_count_func(adv)\n",
    "    # print(f\"ADV: {adv}\")\n",
    "    lime_exp = explainer.explain_instance(adv, pipe.predict_proba, top_labels=1,num_features=wc)\n",
    "    adv_label = lime_exp.top_labels[0]\n",
    "    adv_attributions = lime_exp.as_list(adv_label)\n",
    "    adv_attrib_score = get_attribution_score(adv_attributions) \n",
    "    if original_label != adv_label:\n",
    "        misclassified = True\n",
    "    else:\n",
    "        misclassified = False\n",
    "\n",
    "    return {'Original Label': original_label, 'Original Attribution Score': original_attribution_score,\n",
    "             'Original Text': text, 'New Label': adv_label, 'New Attribution Score': adv_attrib_score, \n",
    "             'New text': adv, 'Misclassified': misclassified}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R1</th>\n",
       "      <th>R2</th>\n",
       "      <th>R3</th>\n",
       "      <th>R4</th>\n",
       "      <th>R5</th>\n",
       "      <th>R6</th>\n",
       "      <th>R7</th>\n",
       "      <th>R8</th>\n",
       "      <th>R9</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     R1   R2   R3   R4   R5   R6   R7   R8   R9 Mean  Var\n",
       "2  0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.00\n",
       "3  0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.00\n",
       "5  0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.00\n",
       "8  0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.00\n",
       "11 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.00\n",
       "10 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.00\n",
       "7  0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.00\n",
       "9  0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.00\n",
       "4  0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.00\n",
       "13 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.00\n",
       "12 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.00\n",
       "6  0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.00\n",
       "14 0.33 0.32 0.33 0.32 0.33 0.33 0.33 0.33 0.32 0.33 0.00\n",
       "19 0.33 0.32 0.33 0.33 0.33 0.33 0.32 0.33 0.32 0.33 0.00\n",
       "16 0.33 0.33 0.33 0.32 0.33 0.32 0.33 0.32 0.33 0.32 0.00\n",
       "20 0.32 0.33 0.32 0.33 0.32 0.32 0.32 0.32 0.32 0.32 0.00\n",
       "18 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.00\n",
       "15 0.32 0.33 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.00\n",
       "23 0.32 0.32 0.32 0.32 0.32 0.32 0.33 0.32 0.32 0.32 0.00\n",
       "21 0.32 0.32 0.32 0.32 0.32 0.32 0.33 0.32 0.32 0.32 0.00\n",
       "17 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.00\n",
       "24 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.00\n",
       "22 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.00"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repetitions = ['R{}'.format(i) for i in range(1,10)]\n",
    "\n",
    "SIL_results = pd.DataFrame( index = range(2,25), \n",
    "                    columns= repetitions)\n",
    "\n",
    "\n",
    "for n_cluster in SIL_results.index:\n",
    "    for col in SIL_results.columns:\n",
    "        algort = KMeans(n_clusters=n_cluster, random_state=42).fit(tfidf_full_svd)\n",
    "        SIL_results.at[n_cluster,col] = silhouette_score(tfidf_full_svd,algort.labels_)\n",
    "        \n",
    "SIL_results['Mean'] = SIL_results[repetitions].mean(axis=1)\n",
    "\n",
    "SIL_results['Var'] = SIL_results[repetitions].var(axis=1)\n",
    "SIL_results.sort_values('Mean',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.3435989444274543\n",
      "CH INDEX: 4264.073737946438\n",
      "DB INDEX: 0.9103115415975793\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 5\n",
    "km = KMeans(n_clusters=num_clusters, n_init=10, random_state=42)\n",
    "km.fit(tfidf_full_svd)\n",
    "silhouette_avg = silhouette_score(tfidf_full_svd, km.labels_)\n",
    "print('Silhouette Score:', silhouette_avg)\n",
    "print(f\"CH INDEX: {calinski_harabasz_score(tfidf_full_svd, km.labels_)}\")\n",
    "print(f\"DB INDEX: {davies_bouldin_score(tfidf_full_svd, km.labels_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>cluster</th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1</td>\n",
       "      <td>the internet may be overflowing with new techn...</td>\n",
       "      <td>candidate00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "      <td>the us postal service announced wednesday a pl...</td>\n",
       "      <td>candidate00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>elementary school students with access to the ...</td>\n",
       "      <td>candidate00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>an influential internet organisation has backe...</td>\n",
       "      <td>candidate00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>an influential internet organisation has backe...</td>\n",
       "      <td>candidate00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>4</td>\n",
       "      <td>britains big banks look set to raise profits b...</td>\n",
       "      <td>candidate00018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "      <td>after two years of hype and euphoria about the...</td>\n",
       "      <td>candidate00047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>czech annual average consumer inflation eased ...</td>\n",
       "      <td>candidate00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>2</td>\n",
       "      <td>kellogg co whose profits for 1996 are under pr...</td>\n",
       "      <td>candidate00037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>londonbased international bank hsbc holdings p...</td>\n",
       "      <td>candidate00018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1  cluster                                               Text   \n",
       "0    0.16  0.04        1  the internet may be overflowing with new techn...  \\\n",
       "1    0.17  0.03        0  the us postal service announced wednesday a pl...   \n",
       "2    0.08 -0.00        1  elementary school students with access to the ...   \n",
       "3    0.15  0.05        1  an influential internet organisation has backe...   \n",
       "4    0.15  0.05        1  an influential internet organisation has backe...   \n",
       "...   ...   ...      ...                                                ...   \n",
       "4995 0.19 -0.08        4  britains big banks look set to raise profits b...   \n",
       "4996 0.21  0.03        0  after two years of hype and euphoria about the...   \n",
       "4997 0.08 -0.01        1  czech annual average consumer inflation eased ...   \n",
       "4998 0.18 -0.03        2  kellogg co whose profits for 1996 are under pr...   \n",
       "4999 0.14 -0.10        2  londonbased international bank hsbc holdings p...   \n",
       "\n",
       "              Author  \n",
       "0     candidate00001  \n",
       "1     candidate00001  \n",
       "2     candidate00001  \n",
       "3     candidate00001  \n",
       "4     candidate00001  \n",
       "...              ...  \n",
       "4995  candidate00018  \n",
       "4996  candidate00047  \n",
       "4997  candidate00002  \n",
       "4998  candidate00037  \n",
       "4999  candidate00018  \n",
       "\n",
       "[5000 rows x 5 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf_full_svd)\n",
    "tfidf_df['cluster'] = km.labels_\n",
    "tfidf_df['Text'] = full_data['Text']\n",
    "tfidf_df['Author'] = full_data['Author']\n",
    "tfidf_df\n",
    "# tfidf_df.to_csv('data\\c50_tfidf_clustered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 1000, 1000, 3000, 1000, 1000)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FOR FULL_DATA\n",
    "# X = tfidf_df.iloc[:,:2]#drop(tfidf_df.columns[:-2], axis=1)#full_data['Text']#.drop(['Author', 'encoded_author'], axis=1)\n",
    "X = tfidf_df['Text']\n",
    "y = tfidf_df['cluster']\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split train into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "len(X_train), len(X_test), len(X_val), len(y_train), len(y_test), len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test =X_test.reset_index(drop=True)\n",
    "y_test =y_test.reset_index(drop=True)\n",
    "\n",
    "X_val =X_val.reset_index(drop=True)\n",
    "y_val =y_val.reset_index(drop=True)\n",
    "\n",
    "X_train =X_train.reset_index(drop=True)\n",
    "y_train =y_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:05<00:00,  5.05it/s]\n"
     ]
    }
   ],
   "source": [
    "lc = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = lc.fit(pd.DataFrame(X_train), pd.DataFrame(X_test), list(y_train), list(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ExtraTreesClassifier</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>None</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.97</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelPropagation</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelSpreading</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingClassifier</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuadraticDiscriminantAnalysis</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.96</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.96</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>None</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearDiscriminantAnalysis</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "      <td>None</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSVC</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>None</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CalibratedClassifierCV</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.91</td>\n",
       "      <td>None</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.91</td>\n",
       "      <td>None</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearestCentroid</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>None</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "      <td>None</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveClassifier</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>None</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.80</td>\n",
       "      <td>None</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifier</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.74</td>\n",
       "      <td>None</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifierCV</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.74</td>\n",
       "      <td>None</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.60</td>\n",
       "      <td>None</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.52</td>\n",
       "      <td>None</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.20</td>\n",
       "      <td>None</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  Balanced Accuracy ROC AUC  F1 Score   \n",
       "Model                                                                          \n",
       "ExtraTreesClassifier               0.99               0.99    None      0.99  \\\n",
       "RandomForestClassifier             0.98               0.98    None      0.98   \n",
       "SVC                                0.98               0.98    None      0.98   \n",
       "XGBClassifier                      0.98               0.98    None      0.98   \n",
       "DecisionTreeClassifier             0.98               0.98    None      0.98   \n",
       "LGBMClassifier                     0.98               0.97    None      0.98   \n",
       "LabelPropagation                   0.97               0.97    None      0.97   \n",
       "LabelSpreading                     0.97               0.97    None      0.97   \n",
       "BaggingClassifier                  0.97               0.97    None      0.97   \n",
       "KNeighborsClassifier               0.97               0.97    None      0.97   \n",
       "LogisticRegression                 0.97               0.97    None      0.97   \n",
       "QuadraticDiscriminantAnalysis      0.97               0.96    None      0.97   \n",
       "GaussianNB                         0.97               0.96    None      0.97   \n",
       "ExtraTreeClassifier                0.95               0.95    None      0.95   \n",
       "LinearDiscriminantAnalysis         0.96               0.94    None      0.96   \n",
       "NuSVC                              0.93               0.92    None      0.93   \n",
       "CalibratedClassifierCV             0.90               0.91    None      0.89   \n",
       "LinearSVC                          0.89               0.91    None      0.89   \n",
       "NearestCentroid                    0.89               0.88    None      0.89   \n",
       "SGDClassifier                      0.86               0.87    None      0.86   \n",
       "PassiveAggressiveClassifier        0.85               0.86    None      0.85   \n",
       "Perceptron                         0.78               0.80    None      0.76   \n",
       "RidgeClassifier                    0.76               0.74    None      0.75   \n",
       "RidgeClassifierCV                  0.76               0.74    None      0.75   \n",
       "BernoulliNB                        0.64               0.60    None      0.60   \n",
       "AdaBoostClassifier                 0.56               0.52    None      0.46   \n",
       "DummyClassifier                    0.29               0.20    None      0.13   \n",
       "\n",
       "                               Time Taken  \n",
       "Model                                      \n",
       "ExtraTreesClassifier                 0.23  \n",
       "RandomForestClassifier               0.35  \n",
       "SVC                                  0.10  \n",
       "XGBClassifier                        0.32  \n",
       "DecisionTreeClassifier               0.01  \n",
       "LGBMClassifier                       0.33  \n",
       "LabelPropagation                     0.22  \n",
       "LabelSpreading                       0.31  \n",
       "BaggingClassifier                    0.06  \n",
       "KNeighborsClassifier                 0.03  \n",
       "LogisticRegression                   0.04  \n",
       "QuadraticDiscriminantAnalysis        0.01  \n",
       "GaussianNB                           0.01  \n",
       "ExtraTreeClassifier                  0.01  \n",
       "LinearDiscriminantAnalysis           0.01  \n",
       "NuSVC                                0.45  \n",
       "CalibratedClassifierCV               0.29  \n",
       "LinearSVC                            0.06  \n",
       "NearestCentroid                      0.01  \n",
       "SGDClassifier                        0.03  \n",
       "PassiveAggressiveClassifier          0.02  \n",
       "Perceptron                           0.02  \n",
       "RidgeClassifier                      0.01  \n",
       "RidgeClassifierCV                    0.01  \n",
       "BernoulliNB                          0.01  \n",
       "AdaBoostClassifier                   0.18  \n",
       "DummyClassifier                      0.01  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.901"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = TfidfVectorizer()#(min_df=3, stop_words='english', ngram_range=(1, 2))\n",
    "svd = TruncatedSVD(n_components=2, n_iter=7, random_state=42)\n",
    "clf = SVC(probability=True)#C=150, gamma=2e-2, probability=True)\n",
    "\n",
    "pipe = make_pipeline(vec, svd, clf)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      australian life office national mutual holding...\n",
       "1      fresh from a highcourt victory floridas antito...\n",
       "2      calenergy inc upped the ante in its hostile pu...\n",
       "3      united news amp media on tuesday secured victo...\n",
       "4      leading chinese dissident liu xiaobo has been ...\n",
       "                             ...                        \n",
       "995    the story of brex minerals ltd and its fabulou...\n",
       "996    compaq computer corp the worlds largest person...\n",
       "997    liggett groups pact breaking away from other c...\n",
       "998    australias westpac banking corp struggling lik...\n",
       "999    some 120 eurotunnel sa plc employees are worki...\n",
       "Name: Text, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = tfidf_df['cluster'].unique()\n",
    "explainer = LimeTextExplainer(class_names=classes)\n",
    "exp_text = X_test[9]\n",
    "actual_auth = y_test[9]\n",
    "lime_exp = explainer.explain_instance(exp_text, pipe.predict_proba, top_labels=1, num_features=word_count_func(exp_text))#pipe.predict_proba)#, num_features=(len(exp_text)//5))\n",
    "# pipe.predict_proba(exp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lime_exp.show_in_notebook()\n",
    "pred = lime_exp.top_labels\n",
    "# actual_auth, \n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('donor', 0.07305992431982994),\n",
       " ('and', -0.050646822323964544),\n",
       " ('to', -0.04739261648396057),\n",
       " ('blessing', 0.040956222865821686),\n",
       " ('in', -0.03781632012805696),\n",
       " ('with', -0.02821373660367552),\n",
       " ('loan', 0.024210585970232266),\n",
       " ('government', -0.02285278957981466),\n",
       " ('exporters', 0.017985306136978855),\n",
       " ('into', -0.016747851789042503),\n",
       " ('cocoa', 0.014500310367551242),\n",
       " ('come', 0.014039282435750864),\n",
       " ('line', 0.012731659459068057),\n",
       " ('conditions', 0.011423320017739082),\n",
       " ('coasts', 0.011400234520894802),\n",
       " ('auction', 0.011389560867105422),\n",
       " ('need', 0.010413243769381847),\n",
       " ('coffee', 0.010359718702811967),\n",
       " ('say', 0.009752780777860544),\n",
       " ('effect', 0.008865018662490254),\n",
       " ('system', 0.00781456380104643),\n",
       " ('revisions', -0.007790155967675714),\n",
       " ('have', -0.007111572724896516),\n",
       " ('ivory', 0.005185549114038121),\n",
       " ('sources', -0.004357614220335949),\n",
       " ('only', 0.0032038974379256747),\n",
       " ('been', 0.0013145931984821144),\n",
       " ('finalised', 0.0003745688759115644)]"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lime_exp = explainer.explain_instance(exp_text, pipe.predict_proba, top_labels=1,num_features=word_count_func(exp_text))\n",
    "pred = lime_exp.top_labels[0]\n",
    "\n",
    "word_attributions = lime_exp.as_list(pred) #list of tuples\n",
    "word_attributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = get_topk(exp_text, word_attributions)\n",
    "# top_words\n",
    "lime_exp.top_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'donor': ['giver', 'bestower', 'presenter', 'conferrer'],\n",
       " 'blessing': ['grace', 'thanksgiving', 'hallow', 'sanctify', 'boon'],\n",
       " 'need': ['motivation', 'indigence', 'want', 'pauperism', 'motive'],\n",
       " 'loan': ['lend', 'loanword'],\n",
       " 'cocoa': ['drinking chocolate', 'chocolate', 'hot chocolate'],\n",
       " 'come': ['cum', 'arrive', 'amount', 'occur', 'descend'],\n",
       " 'auction': ['auction bridge',\n",
       "  'vendue',\n",
       "  'auctioneer',\n",
       "  'auction sale',\n",
       "  'auction off'],\n",
       " 'effect': ['impression', 'issue', 'result', 'outcome', 'event'],\n",
       " 'exporters': ['exporter'],\n",
       " 'coasts': ['glide', 'coast', 'sea-coast', 'seacoast', 'slide'],\n",
       " 'conditions': ['status',\n",
       "  'weather condition',\n",
       "  'stipulation',\n",
       "  'circumstance',\n",
       "  'condition'],\n",
       " 'coffee': ['java', 'burnt umber', 'chocolate', 'deep brown', 'coffee berry'],\n",
       " 'ivory': ['bone', 'tusk', 'pearl', 'off-white'],\n",
       " 'say': ['aver', 'allege', 'articulate', 'read', 'sound out'],\n",
       " 'revisions': ['revise', 'rescript', 'rewrite', 'revision', 'alteration'],\n",
       " 'system': ['organization',\n",
       "  'organisation',\n",
       "  'arrangement',\n",
       "  'scheme',\n",
       "  'system of rules'],\n",
       " 'sources': ['beginning', 'germ', 'author', 'seed', 'source'],\n",
       " 'finalised': ['finalise', 'finalize', 'nail down', 'settle']}"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = get_attribution_score(word_importances)\n",
    "\n",
    "syns = get_syn_dict_wn(top_words)\n",
    "syns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27304\\4264838484.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msub_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msyns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27304\\2377853526.py\u001b[0m in \u001b[0;36msub_words\u001b[1;34m(original_text, synonym_dict, original_label, original_attribution_score)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mlime_exp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplain_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                 \u001b[0mword_attributions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlime_exp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[1;31m# word_attributions = cls_explainer(test_text) #reclassify with replacement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\sadai\\Desktop\\AuthorPrediction\\pt_lm\\lib\\site-packages\\lime\\explanation.py\u001b[0m in \u001b[0;36mas_list\u001b[1;34m(self, label, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \"\"\"\n\u001b[0;32m    140\u001b[0m         \u001b[0mlabel_to_use\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"classification\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdummy_label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[0mans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain_mapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_exp_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_exp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel_to_use\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[0mans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mans\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "res = sub_words(exp_text, syns, pred, score)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_func(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS!\n",
      "ADV: Aussie life office national mutual holdings ltd is bear to make a firm historic debut on the Aussie and new zealand bourses on tuesday after its institutional offer closed six times oversubscribed last week\n",
      "Unable to change label.\n",
      "ADV: fresh from a highcourt triumph floridas antitobacco activist are fresh for a legislative assault on the innovative legal philosophy allow the state government to Eugene Sue cigarette makers\n",
      "SUCCESS!\n",
      "ADV: calenergy Iraqi National Congress upped the ante in its hostile pursuit of british power company northern electric plc friday raising its offer to 782 one thousand thousand pounds 129 billion and setting a dec 20 deadline\n",
      "SUCCESS!\n",
      "ADV: united news amp media on tuesday secured victory in a battle for control of international exhibitions group blenheim with an agreed offer valuing the company at 5925 one thousand thousand pounds 935 one thousand thousand\n",
      "SUCCESS!\n",
      "ADV: leading Taiwanese dissident liu xiaobo has been ordered to serve three years in a labour camp just hours after police detained him in an early morning raid on his beijing home his wife said on wednesday\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Label</th>\n",
       "      <th>Original Attribution Score</th>\n",
       "      <th>Original Text</th>\n",
       "      <th>New Label</th>\n",
       "      <th>New Attribution Score</th>\n",
       "      <th>New text</th>\n",
       "      <th>Misclassified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1.10</td>\n",
       "      <td>australian life office national mutual holding...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>Aussie life office national mutual holdings lt...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>fresh from a highcourt victory floridas antito...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>fresh from a highcourt triumph floridas antito...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1.22</td>\n",
       "      <td>calenergy inc upped the ante in its hostile pu...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.69</td>\n",
       "      <td>calenergy Iraqi National Congress upped the an...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.33</td>\n",
       "      <td>united news amp media on tuesday secured victo...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.44</td>\n",
       "      <td>united news amp media on tuesday secured victo...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1.03</td>\n",
       "      <td>leading chinese dissident liu xiaobo has been ...</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>leading Taiwanese dissident liu xiaobo has bee...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Original Label  Original Attribution Score   \n",
       "0               2                        1.10  \\\n",
       "1               1                        0.04   \n",
       "2               4                        1.22   \n",
       "3               4                        1.33   \n",
       "4               3                        1.03   \n",
       "\n",
       "                                       Original Text  New Label   \n",
       "0  australian life office national mutual holding...          0  \\\n",
       "1  fresh from a highcourt victory floridas antito...          1   \n",
       "2  calenergy inc upped the ante in its hostile pu...          2   \n",
       "3  united news amp media on tuesday secured victo...          2   \n",
       "4  leading chinese dissident liu xiaobo has been ...          1   \n",
       "\n",
       "   New Attribution Score                                           New text   \n",
       "0                   0.53  Aussie life office national mutual holdings lt...  \\\n",
       "1                   0.04  fresh from a highcourt triumph floridas antito...   \n",
       "2                   0.69  calenergy Iraqi National Congress upped the an...   \n",
       "3                   0.44  united news amp media on tuesday secured victo...   \n",
       "4                  -0.08  leading Taiwanese dissident liu xiaobo has bee...   \n",
       "\n",
       "   Misclassified  \n",
       "0           True  \n",
       "1          False  \n",
       "2           True  \n",
       "3           True  \n",
       "4           True  "
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for text in X_test[:5]:\n",
    "    row = generate_adv(text)\n",
    "    # print(row)\n",
    "    results.loc[len(results)] = row\n",
    "    # results.append(row, ignore_index=True)\n",
    "results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R1</th>\n",
       "      <th>R2</th>\n",
       "      <th>R3</th>\n",
       "      <th>R4</th>\n",
       "      <th>R5</th>\n",
       "      <th>R6</th>\n",
       "      <th>R7</th>\n",
       "      <th>R8</th>\n",
       "      <th>R9</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     R1   R2   R3   R4   R5   R6   R7   R8   R9 Mean  Var\n",
       "3  0.43 0.43 0.43 0.43 0.43 0.43 0.43 0.43 0.43 0.43 0.00\n",
       "2  0.43 0.43 0.43 0.43 0.43 0.43 0.43 0.43 0.43 0.43 0.00\n",
       "6  0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.00\n",
       "7  0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.00\n",
       "5  0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.00\n",
       "4  0.40 0.40 0.39 0.40 0.40 0.40 0.39 0.40 0.40 0.40 0.00\n",
       "8  0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.00\n",
       "11 0.38 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.00\n",
       "12 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.00\n",
       "10 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.00\n",
       "9  0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.00\n",
       "13 0.38 0.39 0.37 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.00\n",
       "14 0.37 0.38 0.38 0.38 0.37 0.37 0.38 0.37 0.37 0.37 0.00\n",
       "15 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.00\n",
       "16 0.36 0.36 0.36 0.36 0.36 0.36 0.35 0.36 0.37 0.36 0.00\n",
       "17 0.36 0.36 0.35 0.35 0.36 0.35 0.36 0.35 0.35 0.35 0.00\n",
       "19 0.35 0.35 0.36 0.35 0.35 0.35 0.35 0.35 0.34 0.35 0.00\n",
       "18 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.00\n",
       "20 0.35 0.35 0.35 0.35 0.35 0.34 0.34 0.35 0.35 0.35 0.00\n",
       "23 0.34 0.35 0.34 0.34 0.33 0.35 0.34 0.35 0.34 0.34 0.00\n",
       "24 0.34 0.33 0.34 0.33 0.35 0.34 0.35 0.35 0.34 0.34 0.00\n",
       "21 0.34 0.35 0.36 0.34 0.34 0.34 0.34 0.33 0.34 0.34 0.00\n",
       "22 0.33 0.34 0.35 0.34 0.35 0.33 0.34 0.34 0.33 0.34 0.00"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repetitions = ['R{}'.format(i) for i in range(1,10)]\n",
    "\n",
    "SIL_results = pd.DataFrame( index = range(2,25), \n",
    "                    columns= repetitions)\n",
    "\n",
    "\n",
    "for n_cluster in SIL_results.index:\n",
    "    for col in SIL_results.columns:\n",
    "        algort = KMeans(n_clusters=n_cluster, repetitions = ['R{}'.format(i) for i in range(1,10)]\n",
    "\n",
    "SIL_results = pd.DataFrame( index = range(2,25), \n",
    "                    columns= repetitions)\n",
    "\n",
    "\n",
    "for n_cluster in SIL_results.index:\n",
    "    for col in SIL_results.columns:\n",
    "        algort = KMeans(n_clusters=n_cluster, random_state=42).fit(tfidf_full_svd)\n",
    "        SIL_results.at[n_cluster,col] = silhouette_score(tfidf_full_svd,algort.labels_)\n",
    "        \n",
    "SIL_results['Mean'] = SIL_results[repetitions].mean(axis=1)\n",
    "\n",
    "SIL_results['Var'] = SIL_results[repetitions].var(axis=1)\n",
    "SIL_results.sort_values('Mean',ascending=False)).fit(w2v_full)\n",
    "        SIL_results.at[n_cluster,col] = silhouette_score(w2v_full,algort.labels_)\n",
    "        \n",
    "SIL_results['Mean'] = SIL_results[repetitions].mean(axis=1)\n",
    "\n",
    "SIL_results['Var'] = SIL_results[repetitions].var(axis=1)\n",
    "SIL_results.sort_values('Mean',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.41612533\n",
      "CH INDEX: 5402.987022039822\n",
      "DB INDEX: 0.7978002245266111\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 6\n",
    "km = KMeans(n_clusters=num_clusters, n_init=10, random_state=42)\n",
    "km.fit(w2v_full)\n",
    "silhouette_avg = silhouette_score(w2v_full, km.labels_)\n",
    "print('Silhouette Score:', silhouette_avg)\n",
    "print(f\"CH INDEX: {calinski_harabasz_score(w2v_full, km.labels_)}\")\n",
    "print(f\"DB INDEX: {davies_bouldin_score(w2v_full, km.labels_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 3, ..., 1, 1, 3])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>cluster</th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.37</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>4</td>\n",
       "      <td>the internet may be overflowing with new techn...</td>\n",
       "      <td>candidate00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>the us postal service announced wednesday a pl...</td>\n",
       "      <td>candidate00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>elementary school students with access to the ...</td>\n",
       "      <td>candidate00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "      <td>an influential internet organisation has backe...</td>\n",
       "      <td>candidate00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "      <td>an influential internet organisation has backe...</td>\n",
       "      <td>candidate00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>4</td>\n",
       "      <td>britains big banks look set to raise profits b...</td>\n",
       "      <td>candidate00018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>after two years of hype and euphoria about the...</td>\n",
       "      <td>candidate00047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>czech annual average consumer inflation eased ...</td>\n",
       "      <td>candidate00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>1</td>\n",
       "      <td>kellogg co whose profits for 1996 are under pr...</td>\n",
       "      <td>candidate00037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3</td>\n",
       "      <td>londonbased international bank hsbc holdings p...</td>\n",
       "      <td>candidate00018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1  cluster                                               Text   \n",
       "0    0.37 -0.07        4  the internet may be overflowing with new techn...  \\\n",
       "1    0.34  0.01        0  the us postal service announced wednesday a pl...   \n",
       "2    0.28  0.01        3  elementary school students with access to the ...   \n",
       "3    0.33  0.02        0  an influential internet organisation has backe...   \n",
       "4    0.33  0.02        0  an influential internet organisation has backe...   \n",
       "...   ...   ...      ...                                                ...   \n",
       "4995 0.33 -0.04        4  britains big banks look set to raise profits b...   \n",
       "4996 0.35 -0.01        0  after two years of hype and euphoria about the...   \n",
       "4997 0.23 -0.05        1  czech annual average consumer inflation eased ...   \n",
       "4998 0.29 -0.06        1  kellogg co whose profits for 1996 are under pr...   \n",
       "4999 0.26  0.06        3  londonbased international bank hsbc holdings p...   \n",
       "\n",
       "              Author  \n",
       "0     candidate00001  \n",
       "1     candidate00001  \n",
       "2     candidate00001  \n",
       "3     candidate00001  \n",
       "4     candidate00001  \n",
       "...              ...  \n",
       "4995  candidate00018  \n",
       "4996  candidate00047  \n",
       "4997  candidate00002  \n",
       "4998  candidate00037  \n",
       "4999  candidate00018  \n",
       "\n",
       "[5000 rows x 5 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full_data['cluster'] = km.labels_\n",
    "# full_data.reset_index(drop=True)\n",
    "# full_data\n",
    "w2v_df = pd.DataFrame(w2v_full)\n",
    "w2v_df['cluster'] = km.labels_\n",
    "w2v_df['Text'] = full_data['Text']\n",
    "w2v_df['Author'] = full_data['Author']\n",
    "w2v_df#['cluster'].value_counts()\n",
    "# w2v_df.to_csv('data\\c50_w2v_clustered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster\n",
       "1    1361\n",
       "2    1211\n",
       "0    1090\n",
       "4     685\n",
       "3     653\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counts = full_data['cluster'].value_counts()\n",
    "# counts\n",
    "tfidf_df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 1000, 1000, 3000, 1000, 1000)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FOR FULL_DATA\n",
    "X = w2v_df.drop(w2v_df.columns[-1], axis=1)#full_data['Text']#.drop(['Author', 'encoded_author'], axis=1)\n",
    "y = w2v_df['cluster']\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split train into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "len(X_train), len(X_test), len(X_val), len(y_train), len(y_test), len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 3614.92it/s]\n"
     ]
    }
   ],
   "source": [
    "lc = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = lc.fit(pd.DataFrame(X_train), pd.DataFrame(X_test), list(y_train), list(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Accuracy, Balanced Accuracy, ROC AUC, F1 Score, Time Taken]\n",
       "Index: []"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "pipe = make_pipeline(clf)\n",
    "pipe.fit(X_train, y_train)\n",
    "test_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.98      0.88       212\n",
      "           1       0.97      0.93      0.95       178\n",
      "           2       0.00      0.00      0.00        51\n",
      "           3       0.87      0.86      0.87       185\n",
      "           4       0.94      1.00      0.97       224\n",
      "           5       0.67      0.66      0.66       150\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.71      0.74      0.72      1000\n",
      "weighted avg       0.81      0.86      0.83      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "print(classification_report(y_test, test_pred,  target_names=['0', '1', '2', '3', '4', '5']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_cluster = pd.read_csv(r'data\\c50_w2v_clustered.csv')\n",
    "tfidf_cluster = pd.read_csv(r'data\\c50_tfidf_clustered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>cluster</th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>w2v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1</td>\n",
       "      <td>the internet may be overflowing with new techn...</td>\n",
       "      <td>candidate00001</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "      <td>the us postal service announced wednesday a pl...</td>\n",
       "      <td>candidate00001</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>elementary school students with access to the ...</td>\n",
       "      <td>candidate00001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>an influential internet organisation has backe...</td>\n",
       "      <td>candidate00001</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>an influential internet organisation has backe...</td>\n",
       "      <td>candidate00001</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>4</td>\n",
       "      <td>britains big banks look set to raise profits b...</td>\n",
       "      <td>candidate00018</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "      <td>after two years of hype and euphoria about the...</td>\n",
       "      <td>candidate00047</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>czech annual average consumer inflation eased ...</td>\n",
       "      <td>candidate00002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>2</td>\n",
       "      <td>kellogg co whose profits for 1996 are under pr...</td>\n",
       "      <td>candidate00037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>londonbased international bank hsbc holdings p...</td>\n",
       "      <td>candidate00018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1  cluster                                               Text   \n",
       "0    0.16  0.04        1  the internet may be overflowing with new techn...  \\\n",
       "1    0.17  0.03        0  the us postal service announced wednesday a pl...   \n",
       "2    0.08 -0.00        1  elementary school students with access to the ...   \n",
       "3    0.15  0.05        1  an influential internet organisation has backe...   \n",
       "4    0.15  0.05        1  an influential internet organisation has backe...   \n",
       "...   ...   ...      ...                                                ...   \n",
       "4995 0.19 -0.08        4  britains big banks look set to raise profits b...   \n",
       "4996 0.21  0.03        0  after two years of hype and euphoria about the...   \n",
       "4997 0.08 -0.01        1  czech annual average consumer inflation eased ...   \n",
       "4998 0.18 -0.03        2  kellogg co whose profits for 1996 are under pr...   \n",
       "4999 0.14 -0.10        2  londonbased international bank hsbc holdings p...   \n",
       "\n",
       "              Author  w2v  \n",
       "0     candidate00001    3  \n",
       "1     candidate00001    4  \n",
       "2     candidate00001    1  \n",
       "3     candidate00001    4  \n",
       "4     candidate00001    4  \n",
       "...              ...  ...  \n",
       "4995  candidate00018    3  \n",
       "4996  candidate00047    4  \n",
       "4997  candidate00002    0  \n",
       "4998  candidate00037    0  \n",
       "4999  candidate00018    1  \n",
       "\n",
       "[5000 rows x 6 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df['w2v'] = w2v_df['cluster']\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>cluster</th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>w2v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1</td>\n",
       "      <td>the internet may be overflowing with new techn...</td>\n",
       "      <td>candidate00001</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "      <td>the us postal service announced wednesday a pl...</td>\n",
       "      <td>candidate00001</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>an influential internet organisation has backe...</td>\n",
       "      <td>candidate00001</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>an influential internet organisation has backe...</td>\n",
       "      <td>candidate00001</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>when a company in california sells a book to a...</td>\n",
       "      <td>candidate00001</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3971</th>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>4</td>\n",
       "      <td>britains big banks look set to raise profits b...</td>\n",
       "      <td>candidate00018</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3972</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "      <td>after two years of hype and euphoria about the...</td>\n",
       "      <td>candidate00047</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3973</th>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>czech annual average consumer inflation eased ...</td>\n",
       "      <td>candidate00002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3974</th>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>2</td>\n",
       "      <td>kellogg co whose profits for 1996 are under pr...</td>\n",
       "      <td>candidate00037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3975</th>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>londonbased international bank hsbc holdings p...</td>\n",
       "      <td>candidate00018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3976 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1  cluster                                               Text   \n",
       "0    0.16  0.04        1  the internet may be overflowing with new techn...  \\\n",
       "1    0.17  0.03        0  the us postal service announced wednesday a pl...   \n",
       "2    0.15  0.05        1  an influential internet organisation has backe...   \n",
       "3    0.15  0.05        1  an influential internet organisation has backe...   \n",
       "4    0.13 -0.00        1  when a company in california sells a book to a...   \n",
       "...   ...   ...      ...                                                ...   \n",
       "3971 0.19 -0.08        4  britains big banks look set to raise profits b...   \n",
       "3972 0.21  0.03        0  after two years of hype and euphoria about the...   \n",
       "3973 0.08 -0.01        1  czech annual average consumer inflation eased ...   \n",
       "3974 0.18 -0.03        2  kellogg co whose profits for 1996 are under pr...   \n",
       "3975 0.14 -0.10        2  londonbased international bank hsbc holdings p...   \n",
       "\n",
       "              Author  w2v  \n",
       "0     candidate00001    3  \n",
       "1     candidate00001    4  \n",
       "2     candidate00001    4  \n",
       "3     candidate00001    4  \n",
       "4     candidate00001    2  \n",
       "...              ...  ...  \n",
       "3971  candidate00018    3  \n",
       "3972  candidate00047    4  \n",
       "3973  candidate00002    0  \n",
       "3974  candidate00037    0  \n",
       "3975  candidate00018    1  \n",
       "\n",
       "[3976 rows x 6 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = tfidf_df[tfidf_df['cluster'] != tfidf_df['w2v']]\n",
    "new_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame({'col1': [0, 1, 2, 3], 'col2': ['a', 'b', 'c', 'd']})\n",
    "df2 = pd.DataFrame({'col1': [0, 3, 4, 0], 'col3': ['x', 'y', 'z', 'w']})\n",
    "\n",
    "merged_df = pd.merge(df1, df2, on='col1')\n",
    "count = len(merged_df)\n",
    "count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
